# 卷积神经网络-介绍

含有卷积层的网络，一般都为2维，具有高和宽2个维度



## 二维互相关运算（Cross-correlation）

虽然卷积层得名于卷积运算，但二维互相关运算更加常用

在二维卷积层中，一个二维输入数组，和一个二维核（kernel），也可以成为过滤器，通过互相关运算，输出一个二维数组

![image-20210905140204365](src/03.卷积神经网络/image-20210905140204365.png)

如图，输入一个3×3的数组，核为2×2，输出的单个元素计算过程为：

0×0+1×1+3×2+4×3 = 19

二维互相关运算从左上方开始，按照从左向右，从上到下的顺序滑动，每次对应元素相乘求和



### 实现二维互相关运算代码

```Python
import torch


def corr2d(x: torch.Tensor, k: torch.Tensor) -> torch.Tensor:
    """
    :param x: 输入数组
    :param k: 卷积核数组
    :return: 输出一个经过二维互相关运算torch
    """
    h, w = k.shape
    y = torch.zeros((x.shape[0] - h + 1, x.shape[1] - w + 1))
    for i in range(y.shape[0]):
        for j in range(y.shape[1]):
            y[i][j] = (x[i:i + h, j: j+w] * k).sum()
    return y

```

位置：Code/CNN/corr2d.py



## 二维卷基层

二维卷基层就是拿输入和卷积层做互相关运算，然后加上标量偏差，得到输出

一般我们随机对卷积核进行初始化。不断迭代卷积核和偏差

### 实现一个二维卷基层

```Python
class Conv2d(torch.nn.Module):
    def __init__(self, kernel_size:(int, int)):
        super(Conv2d, self).__init__()
        # definite kernel
        self.weight = torch.nn.Parameter(torch.randn(kernel_size))
        self.bias = torch.nn.Parameter(torch.randn(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias

```

位置：Code/CNN/Conv2D.py



## 卷基层简单应用-图像边缘检测

1 自定义一个Tensor模拟图像

```Python
# 模拟图像
x = torch.ones((4, 6))
x[:, 2:4] = 0
print(x)
```

打印图像:

![image-20210905152826008](src/03.卷积神经网络/image-20210905152826008.png)



2 构造一个1*2的核,如果横向不一致的话,输出非0 否则为0

```
k = torch.tensor([[1, -1]])
y = corr2d(x, k)
print(y)
```

打印出的y为:

![image-20210906203328340](src/03.卷积神经网络/image-20210906203328340.png)

计算完成后,我们发现由于是横向判断边界,所以少了一列,但是边界值已经被标记出来了

# 案例-通过数据学习核数组

这个案例我们随机生成一个卷积核数组，通过随机梯度下降，不断的优化

```Python
from Code.CNN.Conv2D import *

x = torch.ones((6, 8))
x[2:4, 4:6] = 0
y = corr2d(x, torch.tensor([[1, -1]]))
c = Conv2d(kernel_size=(1, 2))
# 学习次数
step = 200
# 学习率
lr = 0.005

for t in range(step):
    y_hat = c(x)
    loss = ((y_hat - y)**2).sum()
    loss.backward()
    c.weight.data -= lr * c.weight.grad
    c.bias.data -= lr * c.bias.grad

    c.weight.grad.zero_()
    c.bias.grad.zero_()
    if (t + 1) % 5 == 0:
        print("step %d, loss: %.3f" % (t+1, loss.item()))


print(c.weight)
```

输出：

![image-20210906225212226](src/03.卷积神经网络/image-20210906225212226.png)

我们发现经过不断地训练，随机化的卷积核越来越接近[1, -1]



# 互相关运算和卷积运算

实际上，卷积运算就是将数组左右翻转，并上下翻转，然后再求互相关运算，他们如果使用同一个核数组，结果往往不尽相同



# 特征图和感受野

## 特征图（Feature map）

二维卷积层输出的二维数组，可以看成是在空间维度上的某一表征，我们叫他特征图

## 感受野（receptive field）

影响输入x的前向运算的所有可能输入区域（可能大于x的尺寸），叫感受野

![image-20210906231421503](src/03.卷积神经网络/image-20210906231421503.png)

例如：输入中阴影部分的4个元素为输出中第一行第一列的感受野

如果我们在输出Y后再加一层互相关运算，最终输出一个单元素输出，那么，不管是Y还是X都是最终输出的感受野
