# 计算机视觉

## 一、图像增广

在深度卷积神经网络中，我们提到大规模数据集是成功应用深度神经网络的前提。图像增广(image augmentation)技术通过对图像做一系列随机改变，来产生相似但不同的训练样本，从而扩大训练数据集的规模，图像增广的另一种解释是，随机改变训练样本来降低模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以对图像进行不同方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖。或者我们通过调整亮度，色彩等降低模型对色彩的敏感度。



首先我们导入必要的模块

```python
import torch
from torch import nn, optim
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib
from torch.utils.data import DataLoader, Dataset
import torchvision

import os
device = "cuda" if torch.cuda.is_available() else "cpu"
matplotlib.use("TkAgg")

```



### 常用的图像增广方法

我们读取一张形状为400 x 500 的图像作为实验的样例。

```python
ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
img = Image.open(os.path.join(ROOT, "Datasets", "cat1.jpg"))
plt.imshow(img)
plt.show(
```

![image-20220508220539310](src/08.计算机视觉(CV)/image-20220508220539310.png)

下面定义绘图函数`show_images`。

```python
def show_images(imgs, num_rows, num_cols, scale=2):
    fig_size = (num_rows * scale, num_cols * scale)
    _, axes = plt.subplots(num_rows, num_cols, fig_size=fig_size)
    for i in range(num_rows):
        for j in range(num_cols):
            axes[i][j].imshow(imgs[i*num_cols+j])
            axes[i][j].axes.get_xaxis().set_visible(False)
            axes[i][j].axes.get_yaxis().set_visible(False)
    return axes
```

大部分图像增广方法都有一定的随机性。为了方便观察图像增广的效果，我们定义一个辅助函数`apply`，这个函数对输入图像`img`多次运行图像增广方法`aug`并展示所有结果。

```python
def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):
    Y = [aug(img) for _ in range(num_rows * num_cols)]
    show_images(Y, num_rows, num_cols, scale)
```



#### 翻转和裁剪

左右翻转图像通常不会改变图像的类别。他是最早也是最广泛使用的一种，下面我们通过`torchvision.transforms`模块创建`RamdomHorizontalFlip`实例来实现一般概率的图像水平翻转。

```python
apply(img, torchvision.transforms.RandomHorizontalFlip())
plt.show()
```



![image-20220508223511101](src/08.计算机视觉(CV)/image-20220508223511101.png)

上下翻转不如左右翻转通用,但是样例图像来说上下翻转不会造成识别障碍,使用`RandomVerticalFilp`实例来实现一半概率的图像垂直翻转。

```python
apply(img, torchvision.transforms.RandomVerticalFlip())
plt.show()
```

![image-20220508224326263](src/08.计算机视觉(CV)/image-20220508224326263-16520210077411.png)

在我们使用的样例图像中，猫在图像正中间，但是其他情况下可能不是这样。池化层章节中我们提到池化层能够降低卷积层对目标位置的敏感度。除此之外，我们还可以通过对图像随机裁剪来让物体以不用的比例出现在图像不同位置。

下面的代码中，我们每次随机裁剪原面积10% ~ 100%的区域，且该区域的宽和高之比随机取自0.5 ~ 2.0，然后再将该区域的宽和高分别缩到200像素。若无特殊情况说明，本节中a和b之间的随机数指的是从区间[a,b]中随机均匀采样所得到的连续值。

```python
shape_aug = torchvision.transforms.RandomResizedCrop(200, scale=(0.1, 1), ratio=(0.5, 2))
apply(img, shape_aug)
plt.show()
```

![image-20220508230054411](src/08.计算机视觉(CV)/image-20220508230054411.png)



#### 变换颜色

另一类增广方法是变化颜色。我们可以从4个方面改变图像的颜色：**亮度（brightness），对比度（contrast），饱和度（saturation）和色调（hue）**。

* 将亮度随机变化为原图的50% ~ 150%

```python
# 这里的0.5表示亮度为原来的随机(1-0.5, 1+0.5)区间
apply(img, torchvision.transforms.ColorJitter(brightness=0.5))
plt.show()
```

![image-20220508231240292](src/08.计算机视觉(CV)/image-20220508231240292.png)

* 变化图像的色调

```python
apply(img, torchvision.transforms.ColorJitter(hue=0.5))
plt.show()
```

![image-20220508231511293](src/08.计算机视觉(CV)/image-20220508231511293.png)

* 变化图像的对比度

```python
apply(img, torchvision.transforms.ColorJitter(contrast=0.5))
plt.show()
```

![image-20220508231656506](src/08.计算机视觉(CV)/image-20220508231656506.png)



* 随机变化所有颜色

```python
apply(img, torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5))
plt.show()
```

![image-20220508231930369](src/08.计算机视觉(CV)/image-20220508231930369.png)



#### 叠加多个图像增广方法

实际应用中我们会将多个增广方法叠加使用，我们可以通过`Compose`实例将上面的多个方法叠加起来，应用到每张图像上。

```python
augs = torchvision.transforms.Compose([
        torchvision.transforms.RandomHorizontalFlip(),
        torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),
        torchvision.transforms.RandomResizedCrop(200, scale=(0.1, 1), ratio=(0.5, 2))
    ])
plt.show()
```

![image-20220508232349740](src/08.计算机视觉(CV)/image-20220508232349740.png)



### 使用图像增广训练模型

下面我们看一个将图像增广应用在实际训练中的例子，我们使用Cifar-10数据集，而不是Fashion-Mnist。这是因为Fashion-Mnist数据集中的物体位置和尺寸都经过了归一化处理，而Cifar-10数据集中物体的颜色大小区别更加显著。下面展示了CIFAR-10数据集中前32张训练图像。

代码路径:`Code/Example/Img_augment.py`

```python
import os
import torch
import torchvision
from Code.Utils.image_augmentation import show_images, plt


ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
CIFAR_PATH = os.path.join(ROOT, "Datasets", "cifar-10")
all_imgs = torchvision.datasets.CIFAR10(CIFAR_PATH, train=True, download=True)

show_images([all_imgs[i][0] for i in range(32)], 4, 8, scale=0.8)
plt.show()
```

![image-20220509203710967](src/08.计算机视觉(CV)/image-20220509203710967.png)

**为了在预测时获取准确的结果，我们通常把图像增广用在训练样本中，而不在预测时使用**。这里只使用最简单的左右翻转，此外我们使用`ToTensor`将小批量图像转换成pytorch识别的格式，即形状为[批量大小，通道数，长，宽]，值域在0-1且类型为32位的浮点数。



接下来我们定义一个辅助函数来方便读取图像并应用图像增广，有关`DataLoader`的介绍，可参考Fashion-Mnist章节。

```python
num_worker = 0 if sys.platform.startswith("win32") else 4

def load_cifar10(is_train, augs, batchsize, root=CIFAR_PATH):
    dataset = torchvision.datasets.CIFAR10(root, train=is_train, transform=augs, download=True)
    return DataLoader(dataset, batch_size=batchsize, shuffle=is_train, num_workers=num_worker)
```

* 训练

我们使用ResNet-18进行训练，先定义一个train函数

代码路径: `Code/Utils/train.py`

```python

def train_img_aug(train_iter, test_iter, net, loss, optimizer, device, num_epoch):
    net = net.to(device)
    print("training on", device)
    batch_count = 0
    for epoch in range(num_epoch):
        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()
        for X, y in train_iter:
            X = X.to(device)
            y = y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
            train_l_sum += l.cpu().item()
            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()
            n += y.shape[0]
            batch_count += 1
        test_acc = evaluate_acc(test_iter, net)
        print("epoch %d, loss %.4f, train_acc %.3f, test_acc %.3f, time %.1f sec" % (epoch,
                     								train_l_sum/batch_count,
                                                    train_acc_sum/n,
                                                    test_acc,
                                                    time.time() - start))
        
```



然后就可以定义`train_with_data_aug`函数使用图像增广来训练模型了。该函数使用Adam算法作为训练使用的优化算法，然后将图像增广应用于训练数据集上，最后调用`train`方法评估模型。

```python
def train_with_data_aug(train_augs, test_augs, lr=0.01):
    batch_size, net = 256, resnet18(10)
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    loss = torch.nn.CrossEntropyLoss()
    train_iter = load_cifar10(True, train_augs, batch_size)
    test_iter = load_cifar10(False, test_augs, batch_size)
    train(train_iter, test_iter, net, loss, optimizer, device, num_epoch=10)


train_with_data_aug(filp_aug, no_aug)
```

输出:

```
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
training on cuda
epoch 0, loss 2.2002, train_acc 74.449, test_acc 0.399, time 46.1 sec
epoch 1, loss 0.9077, train_acc 43.709, test_acc 0.424, time 46.9 sec
epoch 2, loss 0.5008, train_acc 38.031, test_acc 0.493, time 50.9 sec
epoch 3, loss 0.3474, train_acc 31.640, test_acc 0.506, time 59.8 sec
epoch 4, loss 0.2583, train_acc 27.228, test_acc 0.563, time 59.8 sec
epoch 5, loss 0.2018, train_acc 24.119, test_acc 0.540, time 59.9 sec
epoch 6, loss 0.1605, train_acc 21.832, test_acc 0.601, time 59.8 sec
epoch 7, loss 0.1325, train_acc 19.946, test_acc 0.616, time 59.8 sec
epoch 8, loss 0.1116, train_acc 18.313, test_acc 0.609, time 59.9 sec
epoch 9, loss 0.0951, train_acc 17.008, test_acc 0.664, time 59.9 sec
```





## 二、微调

前面章节中,我们介绍了如何在6万张图像的Fashion-MNSIT数据集上训练模型,我们还描述了学术界当下使用最广泛的大规模数据集ImageNet,他有超过1000万的图像和1000类的物体,然而我们平时接触到的数据集规模通常在这两者之间。

假设我们从图像中识别不同种类的椅子，然后我们将购买链接推荐给客户，一种可能的方法是通过先找出100种常见的椅子，为每种椅子拍摄1000张不同角度的图片，然后在收集到的图片数据集中训练分类模型。这个椅子数据集虽然可能比Fashion-Mnist数据集要大庞大，单样本数仍不及ImageNet数据集中的十分之一。这可能会导致适用于ImageNet数据集的复杂模型在这个椅子数据集上过拟合。同时因为数据量有限，最终训练模型的精度也可能达不到使用的要求。

为了应对上述问题，一个简单的方法就是收集更多数据。然而，收集和标注数据会花费大量的时间和资金。

另外一种方法是迁移学习(transfer learning)。将从源数据集学到的知识迁移到目标数据集上。例如，ImageNet数据集中的数据大多和椅子无关，但在该数据中训练的模型可以抽取较为通用的图像特征，从而能够帮助识别边缘，纹理，形状和物体组成等。这些类似的特征对识别椅子可能也会有效。

本节中介绍一个迁移学习中的常用技术：**微调（fine-tuning）**。如图所示，微调由以下4步组成：

1. 在源数据集（如ImageNet）上预训练一个神经网络模型，即源模型。
2. 创建一个新的神经网络模型，即目标模型。他复制了源模型上除了输出层外所有模型设计及参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于新数据集。我们还假设源模型中的输出层和源数据集中的标签紧密相关，因此在目标模型中不予采用。
3. 为目标模型增加一个和目标数据集类别个数对应的输出层，并随机初始化该层的模型参数。
4. 在目标数据集中训练模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。

![image-20220510204717569](src/08.计算机视觉(CV)/image-20220510204717569.png)

当目标数据集远小于源数据集时，微调有助于提高模型的泛化能力。



### 热狗识别

接下来我们实践一个具体的例子:热狗识别。我们将基于一个小数据集对在ImageNet数据集上训练好的ResNet进行微调。该小数据集包含有数千张包含热狗和不包含热狗的图像。我们将使用微调得来的模型来识别一张图像中是否包含热狗。

首先导入模块，`torchvision`中的`models`包提供了常用的预训练模型。如果希望获取更多的预训练模型，可以使用`pretrained-models.pytorch`库

```
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, Dataset
import torchvision
from torchvision.datasets import ImageFolder
from torchvision import transforms
from torchvision import models
import os
import sys

from Code import DATADIR
from Code.Utils.image_augmentation import show_images, plt


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DATA_DIR = os.path.join(DATADIR, "hotdog", "hotdog")

```

#### 获取数据集

我们使用的热狗数据集是从网上抓取的,它含有1400张包含由热狗的正类图像，和同样数量的包含其他食品的负类图像。各类的1000张图像用于被训练，其余则用于被测试。

我们首先将压缩后的数据集下载到路径`data_dir`下，然后解压，得到`train`和`test`2个目录。这两个目录下均有`hotdog`和`no-hotdog`两个类别文件夹，每个类别文件夹下都是图像。

下载地址:https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/hotdog.zip

```python
train_imgs = ImageFolder(os.path.join(DATA_DIR, "train"))
test_imgs = ImageFolder(os.path.join(DATA_DIR, "test"))
```

我们创建2个`ImageFolder`实例来分别读取训练数据集和测试数据集。

```python
hotdogs = [train_imgs[i][0] for i in range(8)]
not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]
```

下面我们画出前8张正类图像和最后8张负类图像。可以看到他们的大小和高宽比不同。

```python
show_images(hotdogs + not_hotdogs, 2, 8, 1.4)
plt.show()
```



![image-20220510213435868](src/08.计算机视觉(CV)/image-20220510213435868.png)

在训练时,我们先从图像中裁剪出随机大小和随机高宽比的一块区域,然后将该区域缩放成高宽均为224像素的输入。测试时，我们将图像高和宽均缩放为256像素， 然后从中裁剪出高宽为224像素的中心区域作为输入。此外，我们对RGB三个颜色通道的数值做标准化：每个数值减去该通道所有数值的平均值，再除以该通道所有数值的标准差作为输出。

> 在使用预训练模型时，一定要和预训练时做同样的预处理，如果你使用的是torchvision的models，那就要求：All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. 如果你是用的是`pretrained-models.pytorch`仓库，请务必阅读其Readme，其中说了如何预处理。

```python
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
train_augs = transforms.Compose([transforms.RandomResizedCrop(size=224),
                                 transforms.RandomHorizontalFlip(),
                                 transforms.ToTensor(),
                                 normalize])

test_augs = transforms.Compose([transforms.Resize(size=256),
                                transforms.CenterCrop(size=224),
                                transforms.ToTensor(),
                                normalize])
```



### 定义和初始化模型

我们使用在ImageNet上预训练的`resnet18`为源模型，这里指定`pretrained=true`来自动下载并加载预训练的模型和参数。第一次使用时需要联网。

> 不管你使用的是torchvision的`models`还是`pretrained-models.pytorch`仓库，默认都会将与训练好的模型下载到你的home目录下的.torch文件夹。你可以通过修改环境变量`$TORCH_MODEL_ZOO`来更改下载目录：
>
> ```
> export TORCH_MODEL_ZOO="/local/pretrained_models"
> ```

下面打印源模型的成员变量`fc`，作为一个全连接层，他将Resnet最终的全局平均池化层输出变换成ImageNet数据集上1000类的输出。

```python
print(pretrained_net.fc)
```

输出：

```
Linear(in_features=512, out_features=1000, bias=True)
```

> 注:如果使用的是其他模型，那可能没有成员变量`fc`（比如models中的VGG预训练模型），所以正确做法是查看对应模型源码中定义部分，这样既不会出错也可以加深理解。`pretrained-models.pytorch`仓库貌似统一了接口，但是我还是建议使用时查看以下对应的源码。

可见此时`pretrained_net`输出层输出类别数是1000，我们应当修改fc的成我们需要的类别：

```python
pretrained_net.fc = torch.nn.Linear(512, 2)
```

此时,pretrained_net的输出层就被随机初始化了，其它层依然保持着训练好的参数。由于是在很大的ImageNet数据集上训练的，此时的参数已经足够好，因此一般只需要较小的学习率来微调这些参数，而`fc`中的随机初始化参数一般需要更大的学习率从头训练。Pytorch可以方便的对模型的不同部分设置不同的学习参数，我们在下面的代码中将fc的学习率设置为预训练过的部分的10倍。

```python
output_params = list(map(id, pretrained_net.fc.parameters()))
feature_params = filter(lambda p: id(p) not in output_params, pretrained_net.parameters())
lr = 0.01
optimizer = optim.SGD([{"params": feature_params},
                       {"params": pretrained_net.fc.parameters(), "lr": lr * 10}],
                      lr=lr, weight_decay=0.001)
```

> 注：由于我的显卡太拉跨，导致只能使用32的batchsize进行训练，由于缩小了bs会导致样本损失的标准差增大，如果使用大学习率会导致损失值过大，因此此处的lr * 10可以不写，直接使用lr

### 微调模型

我们先定义一个使用微调的训练函数`train_fine_tuning`以便多次使用

```python

def train_fine_tuning(net, optimizer, batch_size=32, num_epoch=5):
    train_iter = DataLoader(ImageFolder(os.path.join(DATA_DIR, "train"),
                            transform=train_augs), batch_size=batch_size, shuffle=True)
    test_iter = DataLoader(ImageFolder(os.path.join(DATA_DIR, "test"),
                           transform=test_augs), batch_size=batch_size)
    loss = torch.nn.CrossEntropyLoss()
    train(train_iter, test_iter, net, loss, optimizer, device, num_epoch)

```

根据前面的设置，我们将以10倍的学习率从头训练目标模型的输出层参数。

```python
train_fine_tuning(pretrained_net, optimizer)
```

输出:

```
training on cuda
epoch 0, loss 0.3585, train_acc 0.838, test_acc 0.920, time 32.5 sec
epoch 1, loss 0.3733, train_acc 0.620, test_acc 0.845, time 31.8 sec
epoch 2, loss 0.1477, train_acc 0.822, test_acc 0.812, time 32.3 sec
epoch 3, loss 0.0826, train_acc 0.868, test_acc 0.912, time 32.3 sec
epoch 4, loss 0.0462, train_acc 0.903, test_acc 0.926, time 32.4 sec
```



## 三、目标检测和边界框

前面的章节中，我们介绍了诸多图像分类的模型。在图像分类任务中，假如图里有一个主体目标，并关注如何识别该目标的类别。然而，很多时候图像里有多个感兴趣的目标，我们不仅想知道他们的类别，还想知道他们在图像中的具体位置。在计算机视觉中，我们称之为目标检测（object detection）或者物体检测。

目标检测在多个领域被广泛使用。无人驾驶中，我们需要通过识别拍摄到的视频图像里的行人，车辆，障碍等信息，用来规划路线。安防领域也会检测危险物品等。

在接下来的几节中，我们将介绍多个深度学习模型。我们先熟悉目标位置这个概念：

* 先加载本节中的示例图像，可以看到左边是一条狗，右边是一只猫。他们是这张图中的2个主要目标。

```python
from PIL import Image

import matplotlib.pyplot as plt
import matplotlib
import os

from Code import ROOT

matplotlib.use("TkAgg")

img = Image.open(os.path.join(ROOT, "Datasets", "catdog.jpg"))
plt.imshow(img)
plt.show()
```



![image-20220511215549817](src/08.计算机视觉(CV)/image-20220511215549817.png)

### 边界框

在目标检测中，我们通常使用边界框（bounding box）来描述目标位置。边界框是一个矩形框，可以由矩形左上角的x轴和y轴与右下角的x,y轴坐标确定。我们根据上面的图的坐标信息来定义猫和狗的边界框。图中的坐标原点在图像的左上角，原点往右和往下代表x轴和y轴的正方向。

```python
dog_bbox, cat_bbox = [60, 45, 378, 516], [400, 112, 655, 493]
```

我们可以在图中将边界画出来，以检查是否准确。画之前，我们定义一个辅助函数`bbox_to_rect`，它将边界框表示成matplotlib的边界框格式。

```python
def bbox_to_rect(bbox, color):
    # 将边界框模式(左上x,左上y,右下x,右下y)转换成matplotlib格式:
    # ((左上x,左上y),宽,高)
    return plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
                         fill=False, edgecolor=color, linewidth=2)
```

我们在边界框加载到图像中,可以看到目标主要轮廓基本在框内。

```python
fig = plt.imshow(img)
fig.axes.add_patch(bbox_to_rect(dog_bbox, "blue"))
fig.axes.add_patch(bbox_to_rect(cat_bbox, "red"))
plt.show()
```

![image-20220511223633647](src/08.计算机视觉(CV)/image-20220511223633647.png)



## 四、锚框

目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整目标边缘从而更加准确的预测目标的边界框（ground-truth bounding box）。不同的模型采用的区域采样方法可能不相同。我们这里介绍其中一种：它以每个像素为中心，生成多个大小和宽高比不同的边界框。这些边界框称为锚框（anchor box）。我们将在后面基于锚框实践目标检测。

代码路径：`Code/CV/anchor_box.py`

先导入相关包：

```python
from PIL import Image
import numpy as np
import math
import torch
import os
import matplotlib
import matplotlib.pyplot as plt

matplotlib.use("TkAgg")
from Code import ROOT
from Code.Utils.bboxes import show_bboxes

print(torch.__version__)

img = Image.open(os.path.join(ROOT, "Datasets", "catdog.jpg"))
w, h = img.size

# 728,561
print("w: %d, h:%d" % (w, h))
```

### 生成多个锚框

假设输入图像高为h，宽为w，我们分别以每个像素的中心生成不同形态的锚框。设大小为$s \in (0, 1]$且宽高比为$r>0$，那么锚框的宽和高将分别为$ws\sqrt{r}$和$hs/\sqrt{r}$。当中心位置给定时，已知宽和高的锚框是确定的。

下面我们分别设定好一组大小$s_1,...,s_n$和一组宽高比$r_1,...,r_m$。如果以每个像素为中心时使用的所有大小与宽高比的组合，输入图像将一共得到$whnm$个锚框。虽然这些锚框可能覆盖了所有的真实边框，但计算复杂度容器过高。因此我们通常只对包含s1或者r1的大小与宽高比的组合感兴趣。即

​																				$(s_1,r_1),(s_1,r_2)...(s_1,r_m),(s_2,r_1),(s_3,r_1)....(s_n,r_1)$

也就是说，以相同像素为中心的锚框数量为$n+m-1$。对于整个输入图像，我们将一共生成$wh(n+m-1)$个锚框。

以上生成锚框的方法实现在下面的`MultiBoxPrior`函数中，指定输入一组大小和一组宽高比，该函数将返回所有的锚框。

> 注：Pytorch官方在torchvision.models.detection.rpn里有一个AnchorGeneration可以用来生成Anchor，但是和这里的不一样，感兴趣的可以看看。

```python
def MultiBoxPrior(feature_map, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5]):
    pairs = []
    for r in ratios:
        pairs.append((sizes[0], math.sqrt(r)))
    for s in sizes[1:]:
        pairs.append([s, math.sqrt(ratios[0])])
    pairs = np.array(pairs)

    ss1 = pairs[:, 0] * pairs[:, 1]  # size * sqrt(ration)
    ss2 = pairs[:, 0] / pairs[:, 1]  # size / sqrt(ration)

    base_anchors = np.stack([-ss1, -ss2, ss1, ss2], axis=1) / 2

    h, w = feature_map.shape[-2:]
    shifts_x = np.arange(0, w) / w
    shifts_y = np.arange(0, h) / h

    shift_x, shift_y = np.meshgrid(shifts_x, shifts_y)
    shift_x = shift_x.reshape(-1)
    shift_y = shift_y.reshape(-1)

    shifts = np.stack((shift_x, shift_y, shift_x, shift_y), axis=1)
    anchors = shifts.reshape((-1, 1, 4)) + base_anchors.reshape((1, -1, 4))
    return torch.tensor(anchors, dtype=torch.float32).view(1, -1, 4)


X = torch.Tensor(1, 3, h, w)
Y = MultiBoxPrior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])
# print(Y.shape) # torch.Size([1, 2042040, 4])

```

我们看到，返回锚框变量Y的形状为[1，锚框个数，4]。将锚框Y的形状变为(图像高，图像宽，以相同像素为中心的锚框个数，4)后，我们就可以通过指定像素位置来获取所有该像素为中心的锚框了。下面的例子里我们访问以(250, 250)为中心的第一个锚框。他有4个元素，分别是锚框左上角的x,y轴坐标和右下角的x,y轴坐标，其中x和y的坐标值已经除以图像的宽和高，因此值域在(0,1)之间。

```python
boxes = Y.reshape((h, w, 5, 4))
# print(boxes[250][250][0][:]) # tensor([-0.0316,  0.0706,  0.7184,  0.8206])
```

> 可以验证一下以上输出对不对：size和ratio分别为0.75和1, 则(归一化后的)宽高均为0.75, 所以输出是正确的（0.75 = 0.7184 + 0.0316 = 0.8206 - 0.0706）。

为了描绘出图像中以某个像素为中心的所有锚框,我们先定义`show_bboxes`函数以便在图像上画出边界框。

```python
def show_boxes(axes, bboxes, labels=None, colors=None):
    def _make_list(obj, default_values=None):
        if obj is None:
            obj=default_values
        elif not isinstance(obj, (list, tuple)):
            obj = [obj]
        return obj

    labels = _make_list(labels)
    colors = _make_list(colors, ["b", "g", "r", "m", "c"])
    for i, bbox in enumerate(bboxes):
        color = colors[i % len(colors)]
        rect = bbox_to_rect(bbox.detach().cpu().numpy(), color)
        axes.add_patch(rect)
        if labels and len(labels) > i:
            text_color = "k" if color == "w" else "w"
            axes.text(rect.xy[0], rect.xy[1], labels[i],
                      va="center", ha="center", fontsize=6, color=text_color,
                      bbox=dict(facecolor=color, lw=0))

```

刚刚我们看到,变量`boxes`中x,y的坐标值分别已除以图像的宽和高。在绘图时，我们需要恢复锚框的原始坐标值，并因此定义了变量`bbox_scale`。现在我们可以画出以(250, 250)为中心的所有锚框了。可以看到，大小为0.75且宽高比为1的锚框较好的覆盖了图像中的狗。

```python
fig = plt.imshow(img)
bbox_scale = torch.tensor([w, h, w, h], dtype=torch.float32)
show_boxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,
           ["s=0.75, r=1", "s=0.75, r=2", "s=0.55, r=0.5", "s=0.5, r=1", "s=0.25, r=1"])
plt.show()
```

![image-20220515210327528](src/08.计算机视觉(CV)/image-20220515210327528.png)

### 交并比

我们刚刚提到，某个锚框较好的覆盖了图中的狗。如果该目标真实边界框已知，这里的"较好"该如何量化呢？一种直观的方法是衡量锚框和真实边框的相似度。我们知道Jaccard函数(Jaccard index)可以衡量两个集合的相似度。给定集合A和B，他们的Jaccard系数即二者交集大小除以二者并集大小：

​																					$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$

实际上，我们可以把边界框内的像素区域看成是像素的集合。如此一来，我们可以用两个边界框像素集合的Jaccard系数衡量这两个边界框的相似度。当衡量两个边界框相似度时，我们通常将Jaccard系数称之为交并比(Intersection over union, IoU)。即两个边框相交面积与向并面积之比。如图：

![image-20220515214411149](src/08.计算机视觉(CV)/image-20220515214411149.png)

交并比取值范围在0和1之间，0表示两个边界框无重合像素，1表示2个边界框相等。

下面我们对其实现：

```python
def compute_intersection(set_1, set_2):
    """
    计算Anchor之间的交集
    anchor表示成(xmin, ymin, xmax, ymax)
    """
    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))
    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))
    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)
    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]

def compute_jaccard(set_1, set_2):
    """计算Anchor之间的jaccard系数"""
    intersection = compute_intersection(set_1, set_2)

    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])
    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])

    union = areas_set_1.qusqueeze(1) + areas_set_2.sqeeze(0) - intersection
    return intersection / union

```

在本节剩余部分，将会使用交并比来衡量锚框与真实边界框的相似度。



### 标注训练集的锚框

在训练集中，我们将每个锚框视为一个训练样本。为了训练目标检测模型，我们需要为每个锚框标注2类标签：一个是锚框所含类别，另一个是真实边框相对锚框的偏移量(offset)。目标检测时，我们首先生成多个锚框，然后为没个锚框预测类别以及偏移量。接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的边界框。

我们知道，在目标检测训练集中，每个图像以标注了真实的边界框以及所含目标类别。在生成锚框之后，我们主要依据与锚框类似的真实边界框的位置和类别为锚框标注。那么如何分为锚框分配与其相似的真实边界框呢？

假设图像中的锚框分别为$A_1,A_2...A_{n_a}$，真实边界框分别为$B_1,B_2...B_{n_b}$，且$n_a \ge n_b$，定义矩阵$X \in \mathbb{R}^{n_a \times n_b}$，其中第i行第j列元素$x_{ij}$为锚框$A_i$与真实边界框$B_j$的交并比。首先，我们找出矩阵X中最大元素，并将该元素的行索引和列索引分别标注为$i_1,j_1$，我们为锚框$A_{i1}$分配真实边界框$B_{j1}$，显然，锚框$A_{i1}$与真实边界框$B_{j1}$在所有的锚框-真实边界框中匹配度最高。接下来，将矩阵X中第$i_1$行和$j_1$列上所有的元素丢弃（包括$i_1,j_1$）。找出矩阵X中剩余的最大元素，并将该元素行索引列索引记为$i_2, j_2$。我们将锚框$A_{i2}分配真实边界框B_{j2}$，再将矩阵X中第$i_2$行和第$j_2$列上所有元素丢弃。此时矩阵X中已有2行2列元素被丢弃。以此类推，直到矩阵X中所有$n_b$列元素全部被丢弃，这个时候，我们已经为$n_b$个锚框各分配了一个真实边界框。接下来，我们只遍历剩余的$n_a - n_b$个锚框：给定其中的锚框$A_i$，根据矩阵X的第i行找到与$A_i$交并比大于预先设定的阈值时，才为锚框$A_i$分配真实边界框$B_j$。

如下图，假设矩阵X中最大值为x~23~,我们将为锚框$A_2$分配真实边界框$B_3$。然后，丢弃2行3列中的所有元素，找出剩余阴影部分的最大元素$x_{71}$，为锚框$A_7$分配真实边界框$B_1$。接着如中间所示，丢弃第7行第1列的元素，找出剩余最大元素$x_{54}$，为锚框$A_5$分配真实边界框$B_4$。最后如图（右）所示，丢弃矩阵中第5行第4列所有元素，找出剩余最大元素$x_{92}$，为锚框$A_9$分配真实边界框$B_2$。之后，我们只需遍历除去A2，A5，A7，A9剩余锚框，并根据阈值判断是否为剩余锚框分配真实边界框。

![image-20220516220642725](src/08.计算机视觉(CV)/image-20220516220642725.png)



现在我们可以标注锚框的类别和偏移量了。如果一个锚框A被分配了真实边界框B，将锚框A的类别设置为B的类别，并根据B和A的中心最表相对位置以及2个框的相对大小为锚框A标注偏移量。由于数据集中各个框的位置和大小各异，因此这些相对位置和相对大小通常要经过一些特殊变换，才能使偏移量分布更加均匀从而更加容易拟合。设锚框将其真实边界框的中心坐标分别为$(x_a, y_a)$和$(x_b, y_b)$，A和B的宽分别为$w_a和w_b$，高分别为$h_a和h_b$，一个常用的技巧是将A的偏移量标注为

​																$\large (\frac{\frac{x_b-x_a}{w_a} - \mu_x}{\sigma_x}, \frac{\frac{y_b-y_a}{h_a} - \mu_y}{\sigma_y}, \frac{log \frac{w_b}{w_a} - \mu_w}{\sigma_w}, \frac{log \frac{h_b}{h_a}-\mu h}{\sigma_h})$

其中常数的默认值$\mu_x=\mu_y=\mu_w=\mu_h=0$，$\sigma_x=\sigma_y=0.1, \sigma_w=\sigma_h=0.2$。如果一个锚框没有被分配真实边界框，我们只需将该锚框类别设置为背景。类别为背景的锚框通常被称为负类锚框。其余则被称为正类锚框。

下面演示一个具体的例子，我们读取图像中的猫和狗的真实边界框，其中第一个元素为类别（0为狗，1为猫），剩余4个元素分别为左上角x和y轴坐标，右下角x和y轴坐标（值域在0到1之间）。这里通过左上角和右下角的坐标构造了5个需要标注的锚框，分别为$A_0,...,A_4$(程序中索引从0开始)。先画出这些锚框和真实边框在图中的位置。

代码路径:`Code/Example/bbox_1.py`

```python
import os
import torch
import matplotlib
import matplotlib.pyplot as plt
from PIL import Image

from Code import ROOT
from Code.Utils.bboxes import show_bboxes

matplotlib.use("TkAgg")
img = Image.open(os.path.join(ROOT, "Datasets", "catdog.jpg"))
w, h = img.size

bbox_scale = torch.tensor((w, h, w, h), dtype=torch.float32)
# 真实边界框:[种类,左上x,左上y,右下x,右下y]
ground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],
                             [1, 0.55, 0.2, 0.9, 0.88]])
# 锚框
anchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],
                        [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],
                        [0.57, 0.3, 0.92, 0.9]])

fig = plt.imshow(img)
show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ["dog", "cat"], "k")
show_bboxes(fig.axes, anchors * bbox_scale, ["0", "1", "2", "3", "4"])
plt.show()
```

![image-20220517221113363](src/08.计算机视觉(CV)/image-20220517221113363.png)

下面实现`MultiBoxTarget`函数来为锚框标注类别和偏移量。该函数将背景类别设为0，并令从零开始的目标类别的整数索引自加1（1为狗，2为猫）。

```python

def assign_anchor(bb, anchor, jaccard_threshold=0.5):
    """
    Params:
        bb: 真实边界框,shape:[nb, 4]
        anchor: 待分配的锚框,shape[na, 4]
        jaccard_threshold: 交并比阈值
    Return:
        assigned_idx: shape:(na, )，每个anchor分配的真实标签索引，如果未分配则为-1
    """
    na = anchor.shape[0]
    nb = bb.shape[0]
    jaccard = compute_jaccard(anchor, bb).detach().cpu().numpy()
    assign_idx = np.ones(na) * -1

    jaccard_cp = jaccard.copy()
    for j in range(nb):
        i = np.argmax(jaccard_cp[:, j])
        assign_idx[i] = j
        jaccard_cp[i, :] = float("-inf")  # 赋值为负无穷，相当于去掉这一行

    # 处理还未被分配的anchor,要求满足jaccard threshold
    for i in range(na):
        if assign_idx[i] == -1:
            j = np.argmax(jaccard[i, :])
            if jaccard[i, j] >= jaccard_threshold:
                assign_idx[i] = j
    return torch.tensor(assign_idx, dtype=torch.long)


def xy_to_cxcy(xy):
    """将(左上x,左上y,右下x,右下y)改成(中心x,中心y,w,h)的形式"""
    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # cx, cy
                      xy[:, 2:] - xy[:, :2]], 1)   # w, h


def MultiBoxTarget(anchor, label):
    """为锚框标注类别和偏移量"""
    assert len(anchor.shape) == 3 and len(label.shape) == 3
    bn = label.shape[0]

    def MultiBoxTarget_one(anc, lab, eps=1e-6):
        an = anc.shape[0]
        assigned_idx = assign_anchor(lab[:, 1:], anc)
        bbox_mask = ((assigned_idx >= 0).float().unsqueeze(-1)).repeat(1, 4)

        cls_labels = torch.zeros(an, dtype=torch.long)
        # 所有anchor对应的bb坐标
        assigned_bb = torch.zeros((an, 4), dtype=torch.float32)
        for i in range(an):
            bb_idx = assigned_idx[i]
            # 非背景
            if bb_idx >= 0:
                cls_labels[i] = lab[bb_idx, 0].long().item() + 1
                assigned_bb[i, :] = lab[bb_idx, 1:]

        center_anc = xy_to_cxcy(anc)
        center_assigned_bb = xy_to_cxcy(assigned_bb)

        offset_xy = 10.0 * (center_assigned_bb[:, :2] - center_anc[:, :2]) / center_anc[:, 2:]
        offset_wh = 5.0 * torch.log(eps + center_assigned_bb[:, 2:] / center_anc[:, 2:])
        offset = torch.cat([offset_xy, offset_wh], dim=1) * bbox_mask

        return offset.view(-1), bbox_mask.view(-1), cls_labels

    batch_offset = []
    batch_mask = []
    batch_cls_labels = []
    for b in range(bn):
        offset, bbox_mask, cls_labels = MultiBoxTarget_one(anchor[0, :, :], label[b, :, :])

        batch_offset.append(offset)
        batch_mask.append(bbox_mask)
        batch_cls_labels.append(cls_labels)

    bbox_offset = torch.stack(batch_offset)
    bbox_mask = torch.stack(batch_mask)
    cls_labels = torch.stack(batch_cls_labels)
    return [bbox_offset, bbox_mask, cls_labels]

```

我们通过`unsqueeze`函数来为锚框和真实边界框添加样本维。

```python
labels = MultiBoxTarget(anchors.unsqueeze(dim=0),
                        ground_truth.unsqueeze(dim=0))
print(labels[2])
```

输出:

```
# 表示为每个锚框标注的类别,0不识别,1狗,2猫
tensor([[0, 1, 2, 0, 2]])
```

我们根据锚框与真实边界框在图像中的位置来分析这些标注的类别。首先，所有的"锚框-真实边界框"配对中锚框A~4~与猫的真实边界框的交并比最大，因此A~4~被标注为猫。不考虑锚框A~4~或猫的真实边界框，在剩余的"锚框-真实边界框"配对中，最大交并比的锚框配对为A1和狗的真实边界框，因此锚框A1被标注为狗。接下来遍历未被标注的剩余3个锚框，与锚框A0交并比最大的真实边界框类别为狗，但小于阈值（0.5），因此被标注为背景；与锚框A2交并比最大的真实边界框为猫，且交并比大于阈值，因此类别标注为猫；与锚框A3交并比最大的真实边界框类别为猫，但交并比小于阈值，因此被标记为背景。

返回值的第二项为掩码(mask)变量，形状为（批量大小，锚框个数的4倍）。掩码中的每个元素和每个锚框的4个偏移量一一对应。由于我们不关心对背景的检测，有关负类偏移量不应影响目标函数。通过按元素乘法，掩码中的0可以在计算目标函数之前过滤掉负类的偏移量。

```python
print(labels[1])
```

输出：

```
tensor([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,
         1., 1.]])
```

返回的第一项是为每个锚框标注的4个偏移量， 其中负类锚框偏移量被标注为0。



### 输出预测边界框

在模型预测阶段，我们先为图像生成多个锚框，并为这些锚框一一预测类型和偏移量。随后，我们我们根据锚框和预测偏移量得到预测边界框。当锚框较多时，同一个目标上可能会出现较多相似的预测边界框。为了使结果更加简洁，我们可以移除相似的边界框。常用的方法为**非极大值抑制（none maximum supression, NMS）**。

我们来描述下非极大值抑制的工作原理。对于一个预测边框B，模型会计算各个类别的预测概率。设其中最大的预测概率为p，该概率对应的类别即为B的类别概率。我们也将p称为预测边界框B的**置信度**。在同一图像上，我们将预测类别非背景的预测边界框置信度从高到低排序，得到列表L。从L中选取置信度最高的预测边界框B~1~作为基准，将所有与B~1~的交并比大于某阈值的非基准预测边界框从L中移除。这里的阈值是预先确定的超参数。此时L保留了置信度最高的预测边界框并移除了与其相似的其他边界框。接下来选取L中的置信度第二高的预测边界框B~2~作为基准，将所有与B~2~的交并比大于某阈值的非基准预测边界框从L中移除。重复这一过程，直到L中所有预测边界框都曾作为基准。此时L中任意一个边框的交并比都小于阈值。最终，输出列表L中的所有预测边框。

下面展示一个具体的例子，先构造4个锚框。简单起见，我们假设预测偏移量全是0：预测边界框即为真实边界框。最后我们构造每个类别的预测概率。

```python
import torch
import os
from PIL import Image

from Code import ROOT
from Code.Utils.bboxes import show_bboxes, plt


anchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],
                        [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])
offset_preds = torch.tensor([0.0] * (4 * len(anchors)))
cls_probs = torch.tensor([[0., 0., 0., 0.],  # 背景的预测概率
                          [0.9, 0.8, 0.7, 0.1],  # 狗的预测概率
                          [0.1, 0.2, 0.3, 0.9]])  # 猫的预测概率
img = Image.open(os.path.join(ROOT, "Datasets", "catdog.jpg"))
bbox_scale = torch.tensor(img.size * 2, dtype=torch.float32)  # (728, 561)
fig = plt.imshow(img)
show_bboxes(fig.axes, anchors*bbox_scale, ["dog=0.9", "dog=0.8", "dog=0.7", "cat=0.9"])
plt.show()
```

![image-20220525220700446](src/08.计算机视觉(CV)/image-20220525220700446.png)

我们在图像框打印出预测边界框和它的置信度。

下面我们实现`MultiBoxDetection`函数来实现非极大值抑制。

```python
from collections import namedtuple
Pred_BB_Info = namedtuple("Pred_BB_Info", ["index", "class_id", "confidence", "xyxy"])


def non_max_suppression(bb_info_list, nms_threshold=0.5):
    """
    非极大抑制处理边界框
    Params:
        bb_info_list: 信息列表，包含置信度，预测类别等信息
    return:
        output: bb_info_list的列表，只保留过滤后的边框信息
    """
    output = []
    sorted_bb_info_list = sorted(bb_info_list, key=lambda x: x.confidence, reverse=True)
    while len(sorted_bb_info_list) != 0:
        best = sorted_bb_info_list.pop(0)
        output.append(best)

        if len(sorted_bb_info_list) == 0:
            break

        bb_xyxy = []
        for bb in sorted_bb_info_list:
            bb_xyxy.append(bb.xyxy)

        iou = compute_jaccard(torch.tensor([best.xyxy]),
                              torch.tensor(bb_xyxy))[0]
        n = len(sorted_bb_info_list)
        sorted_bb_info_list = [sorted_bb_info_list[i] for i in range(n) if iou[i] < nms_threshold]
    return output


def MultiBoxDetection(cls_prob, loc_pred, anchor, nms_threshold=0.5):
    assert len(cls_prob.shape) == 3 and len(loc_pred.shape) == 2 and len(anchor.shape) == 3
    bn = cls_prob.shape[0]

    def MultiBoxDetection_one(c_p, l_p, anc, nms_threshold):
        pred_bb_num = c_p.shape[1]
        anc = (anc + l_p.view(pred_bb_num, 4)).detach().cpu().numpy()

        confidence, class_id = torch.max(c_p, 0)
        confidence = confidence.detach().cpu().numpy()

        pred_bb_info = [Pred_BB_Info(
            index=i,
            class_id=class_id[i] - 1,
            confidence=confidence[i],
            xyxy=[*anc[i]]
        ) for i in range(pred_bb_num)]

        obj_bb_idx = [bb.index for bb in non_max_suppression(pred_bb_info, nms_threshold)]

        output = []
        for bb in pred_bb_info:
            output.append([
                (bb.class_id if bb.index in obj_bb_idx else -1.0),
                bb.confidence,
                *bb.xyxy
            ])
        return torch.tensor(output)

    batch_output = []
    for b in range(bn):
        batch_output.append(MultiBoxDetection_one(cls_prob[b], loc_pred[b], anchor[0], nms_threshold))

    return torch.stack(batch_output)

```

然后我们运行`MultiBoxDetection`函数并设置阈值为0.5，这里为输出都增加了样本维，我们看到，返回结果的形状为（批量大小，锚框个数，6）。其中最后6个元素代表同一个预测边框的输出信息。第一个元素是从零开始计数的预测类别（0为狗，1为猫），其中-1表示在非极大值抑制中被排除。第二个元素是预测边界框的置信度，剩余的4个元素为分别是预测边界框的左上xy和右下xy坐标（值域在0-1之间）。

```python
output = MultiBoxDetection(
    cls_probs.unsqueeze(dim=0), offset_preds.unsqueeze(0),
    anchors.unsqueeze(0), nms_threshold=0.5
)
print(output)
```

输出：

![image-20220526205919537](src/08.计算机视觉(CV)/image-20220526205919537.png)

我们移除掉类别为-1的边界框，并可视化结果：

```python
fig = plt.imshow(img)
for i in output[0].detach().cpu().numpy():
    if i[0] == -1:
        continue
    label = ("dog=", "cat=")[int(i[0])] + str(i[1])
    show_bboxes(fig.axes, [torch.tensor(i[2:]) * bbox_scale], label)
plt.show()
```



![image-20220526220640767](src/08.计算机视觉(CV)/image-20220526220640767.png)

实践中,我们可以在执行非极大值抑制前将置信度较低的预测边界框移除,从而减小非极大值抑制的计算量。我们还可以筛选非极大值抑制的输出，例如，只保留其中置信度较高的结果作为最终输出。



## 五、多尺度目标检测

在第四章中，我们实验以输入图像的每个像素为中心生成多个锚框。这些锚框是对输入图像不同区域的采样。然而，如果以图像每个像素为中心都生成锚框，很容易因为生成的锚框数量过多从而导致计算量过大。举个例子，假设输入图像的高和宽分别为561和728像素，如果以每个像素为中心生成5个锚框。那么一张图像上将会标注超过200万个锚框(561 * 728 * 5)。

减少锚框个数并不难，一种简单的方法是从输入图像中均匀的采样一小部分像素，并以采样的像素为中心生成锚框，另外，在不同尺度下我们可以生成不同数量不同大小的锚框。值得注意的是，较小目标比较大目标在图像上出现的位置可能更多。举个简单的例子，形状为1x1,1x2,2x2的目标在2x2的图像中出现的位置分别由4,2,1种。因此，当使用较小锚框检测较小目标时，我们可以采样较多的区域。当使用较大锚框采样较大目标时，我们可以采样较少的区域。

为了演示多尺度生成锚框，我们先读取一张图像。它的高和宽分别为561和728像素

```python
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import os
import torch

import sys
from Code import ROOT
from Code.CV.anchor_box import MultiBoxPrior
from Code.Utils.bboxes import show_bboxes

img = Image.open(os.path.join(ROOT, "Datasets", "catdog.jpg"))
w, h = img.size
```

我们在ConvNet章节中介绍过,将卷积神经网络的二维数组称作特征图，我们可以通过定义特征图的形状来确定任意图像上均匀采样的锚框中心。

下面定义`display_anchor`函数。我们在特征图`fmap`上以每个单元为中心生成锚框`anchors`。由于锚框`anchors`中x和y的坐标已经分别除以过图像的宽和高，这些值域在0-1之间的值表示锚框在特征图中的位置。由于锚框`anchors`的中心遍布特征图上的所有单元，`anchors`的中心在任意图像的空间相对位置一定是均匀分布的。具体来说，当特征图的宽和高分别为`fmap_w`和`fmap_h`时，该函数将在任意图像上均匀采样`fmap_w`列和`fmap_h`行个像素。并以他们为中心生成大小为`s`的不同宽高比(ratios)锚框。

```python
def display_anchors(fmap_w, fmap_h, s):
    # 前两维的取值不影响输出结果
    fmap = torch.zeros((1, 10, fmap_h, fmap_w), dtype=torch.float32)

    # 平移所有锚框使之均匀分布与图片中
    offset_w, offset_h = 1.0/fmap_w, 1.0/fmap_h
    anchors = MultiBoxPrior(fmap, sizes=s, ratios=[1, 2, 0.5]) + \
        torch.tensor([offset_w / 2, offset_h / 2, offset_w / 2, offset_h / 2])
    
    bbox_scale = torch.tensor([w, h, w, h], dtype=torch.float32)
    show_bboxes(plt.imshow(img).axes, anchors[0] * bbox_scale)
```

我们先关注小目标的检测，为了在显示时更加容易分辨，这里令不同中心的锚框不重合：设锚框大小为0.15，特征图的高宽分别为2和4。可以看出上面2行4列的锚框分布均匀。

```python
display_anchors(4, 2, [0.15])
plt.show()
```

![image-20220528165616253](src/08.计算机视觉(CV)/image-20220528165616253.png)

我们将特征图的高宽分别减半，并用更大的锚框检测目标。当锚框大小设置为0.4时，某些锚框的区域有重合。

![image-20220528170741185](src/08.计算机视觉(CV)/image-20220528170741185.png)

最后我们将特征图的宽进一步减至1，并将锚框大小设置为0.8。此时的锚框即为图像中心

![image-20220528170849798](src/08.计算机视觉(CV)/image-20220528170849798.png)

既然已经在多个尺度上生成了不同尺度的锚框，相应的，我们需要在不同尺度下检测不同大小的目标。下面我们来介绍一种基于卷积神经网络的方法。

在某个尺度下，假设我们依据$c_i$张形状为$h \times w$的特征图生成$h \times w$组不同中心的锚框，且每组锚框个数为$a$。例如，在刚才实验的第一个尺度下，我们依据10(通道数)张形状为$4 \times 2$的特征图生成了8组不同中心的锚框。且每组含3个锚框。接下来根据真实边界框的类别和位置，每个锚框将被标记类别和偏移量。在当前尺度下，目标检测模型需要根据输入图像预测$w \times h$组不同中心的锚框类别和偏移量。

假设这里的$c_i$张特征图为卷积神经网络根据图像前向计算所得的中间输出。既然每张特征图上都有$w \times h$个不同的空间位置，那么相同空间位置可以看做含有$c_i$个特征单元。根据感受野的定义，特征图在相同空间位置的$c_i$个单元在输入图像上的感受野相同，并表征了同一感受野内的输入图像信息。因此，我们可以将特征图在相同空间的位置的$c_i$个单元变换为以该位置为中心生成的a个锚框的类别和偏移量，不难发现，本质上，我们用输入图像在某个感受野区域内的信息来预测输入图像上与该区域位置相近的锚框的类别和偏移量。

当不同层的特征图在输入图像上分别拥有不同大小的感受野时，他们将分别用来检测大小不同的目标。例如，我们可以通过设计网络，令较为接近输入层的特征网络中每个单元拥有较为宽阔的感受野，从而检测输入图像中更大尺寸的目标（我们将在后面的单发多框检测章节中实现一个多尺度目标检测模型）。



## 六、目标检测数据集

在目标检测领域并没有类似Mnist或者Fashion-Mnist这样的小规模数据集。为了快速测试模型，我们合并了一个小数据集。首先使用一个开源的皮卡丘3D模型生成1000张不同角度不同大小的皮卡丘图片。然后收集一系列的背景图像，并在每张图的随机位置放置一张随机的皮卡丘图像。该数据集使用MXNet提供的im2rec工具将图像转换成了二进制的RecordIO模式。该格式既可以降低数据在磁盘上的存储开销，又能提高读取效率。如果想了解更多图像读取方法，可以查阅GluonCV工具包的文档。



### 下载数据集

前面说了，皮卡丘数据集采用MXNet的工具将图像转换成二进制的格式，但是我们使用torch，所以需要使用脚本将其转换为png格式并用json标注其label信息。继续阅读前，并保证脚本已经被执行且数据已准备好。pikachu文件夹下的文件结构如图所示

![image-20220528183030182](src/08.计算机视觉(CV)/image-20220528183030182.png)

```
cd Code/Other
python download_obj_detection_datasets.py
```

如果执行失败，请参考官方章节，下载最新的数据集（最新的数据集是以香蕉为样本的数据集，下载较为简单）



### 读取数据集

我们先定义一个数据集类`PikachuDetDataset`，数据集每个样本包含label和image，其中label是一个$m \times 5$的向量，即m个边界框，每个边界框是由`[class, x_min, y_min, x_max, y_max]`表示。这里的皮卡丘数据集中每个图像只有1个边界框，因此m=1，image是一个所有元素都处于[0, 1]之间的浮点tensor，代表图片数据。

```python
class PikachuDetDataset(torch.utils.data.Dataset):
    def __init__(self, datadir, part, image_size=(256,256)):
        assert part in ["train", "val"]
        self.image_size = image_size
        self.image_dir = os.path.join(datadir, part, "images")

        with open(os.path.join(datadir, part, "label.json")) as f:
            self.label = json.load(f)

        self.transform = torchvision.transforms.Compose([
            torchvision.transforms.ToTensor()
        ])

    def __len__(self):
        return len(self.label)

    def __getitem__(self, index):
        image_path = str(index + 1) + ".png"

        cls = self.label[image_path]["class"]
        label = np.array([cls] + self.label[image_path]["loc"], dtype="float32")[None, :]
        PIL_img = Image.open(os.path.join(self.image_dir, image_path)).convert("RGB").resize(self.image_size)
        img = self.transform(PIL_img)
        sample = {
            "label": label,
            "image": img
        }

        return sample
```

然后我们通过创建`DataLoader`示例来读取目标检测数据集。我们将以随机顺序读取训练数据集，按顺序读取测试数据集。

注：原书中做了图像增广操作。

```python
def load_data_pikachu(batch_size, edge_size=256, data_dir=os.path.join(ROOT, "Datasets", "pikachu")):
    """edge_size: 输出图像的宽和高"""
    image_size = (edge_size, edge_size)
    train_dataset = PikachuDetDataset(data_dir, "train", image_size)
    val_dataset = PikachuDetDataset(data_dir, "val", image_size)

    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)
    return train_iter, val_iter

```

下面我们读取一个小批量并打印图像的标签和形状，图片的形状和之前实验相同，依然是（批次，通道，高，宽），而标签的形状则是（批量，m，5）。其中m等于数据集中每个图像最多含有的边界框个数，剩下的5个元素表示类别，起始xy和结束xy。

小批量计算虽然高效，但是它要求每个批次含有相同数量的边界框数，由于每张图像可能含有的边界框数不同，我们为边界框个数小于m的图像填充非法边界框。

这里的皮卡丘数据集中每个图像只有1个真实边界框，因此m=1。

```python
batch_size, edge_size = 32, 256
train_iter, _ = load_data_pikachu(batch_size, edge_size)
batch = iter(train_iter).next()
print(batch["image"].shape, batch["label"].shape)
```

打印：

```
torch.Size([32, 3, 256, 256]) torch.Size([32, 1, 5])
```



### 图示数据

我们画出10张图和他们的边界框。可以看到，皮卡丘的角度，大小，位置在每张图都不一样，当然，这只是一个简单的人工数据集。实际中会复杂许多。

```python
imgs = batch["image"][0:10].permute(0, 2, 3, 1)
bboxes = batch["label"][0:10, 0, 1:]
axes = show_images(imgs, 2, 5).flatten()
for ax, bb in zip(axes, bboxes):
    show_bboxes(ax, [bb * edge_size], colors="w")
plt.show()
```

![image-20220530224945693](src/08.计算机视觉(CV)/image-20220530224945693.png)



## 七、单发多框检测

我们设计一种目标检测模型：单发多框检测(SSD)，该模型简单，快速且被广泛使用。尽管这只是其中一种目标监测模型，本节中的设计原则和实现细节也会被应用于其他章节。

### 模型

下图描述了单发多框检测的设计。此模型主要由基础网络组成，其后是几个多尺度特征块。基础网络用于从输入图像中提取特征，因此他可以使用深度卷积神经网络。单发多框检测论文中选用了在分类层（也就是多层感知机）之间前端的VGG，现在常用Resent替代。

我们可以设计基础网络，是它输出的高和宽较大。这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。接下来的每个多尺度特征块将上一层提供的特征图高和宽缩小，并使特征图中每个单元在输入图像上的感受野变得更加广阔。

由于接近下图上部分的多尺度特征块特征图较小，但具有较大的感受野，他们适合检测较少但较大的物体。简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度的目标检测模型。

![image-20220530232513131](src/08.计算机视觉(CV)/image-20220530232513131.png)

下面，我们来介绍不同块的实现细节。

#### 类别预测层

设目标类别的数量为q,锚框有q+1个类别，其中0类是背景，在某个尺度下，设特征图的高和宽分别为h和w，如果以其中的每个单元为中心生成a个锚框，那么我们需要对$hwa$个锚框进行分类。如果使用全连接层作为输出，很容易导致模型参数过多。

类别预测使用一个保持输入高和宽的卷积层，这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。考虑输出和输入统一空间坐标x,y，输出特征图上(x,y)坐标的通道里包含了以输入特征图(x,y)坐标为中心生成的所有锚框的类别预测。因此输出通道数为$a(q+1)$，其中索引为$i(q+1)+j(0 \le j \le q)$的通道代表了索引为i的锚框有关索引为j的预测。

下面，我们定义这样一个类别预测层，通过`num_anchor`和`num_classes`分别指定了a和q。该图层使用填充1的为$3 \times 3$的卷积层。此卷积层的输入和输出宽度和高度保持不变。

```
```

#### 边界框预测层

边界框预测层的设计与类别预测层的设计类似。唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是q+1个类别。

```
```

#### 连接多尺度的预测

正如我们提到的，单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量。在不同的尺度下，特征图的形状或以同一单元为中心的锚框数量可能会有所不同。因此不同尺度下预测输出的形状可能会有所不同。

在以下示例中，我们为同一个小批量构建2个不同比例（Y1和Y2）的特征图，其中Y2的高和宽是Y1的一半。以类别预测为例，假设Y1和Y2的每个单元分别生成了5个和3个锚框，进一步假设目标类别的数量为10，对于特征图Y1和Y2，类别预测输出中的通道数分别为$5*(10+1)=55$和$3 * (10+1) = 33$，其中任意输出的形状是(批量大小，通道数，高度，宽度)

```python
Y1 = forward(torch.zeros(2, 8, 20, 20), cls_predictor(8, 5, 10))
Y2 = forward(torch.zeros(2, 16, 10, 10), cls_predictor(16, 3, 10))
```

output:

```
torch.Size([2, 55, 20, 20]) torch.Size([2, 33, 10, 10])
```

正如我们看到的，除了批量大小这一维度，其他三个维度都具有不同的尺寸。为了将这两个输出连接起来以提高计算效率，我们将把这些张量转换为更一致的格式。

通道维包含中心相同的锚框的预测结果。我们首先将通道维移动到最后一维，因为不同尺度下批量大小仍保持不变，我们可以将预测结果转换为2维模式（批量大小，高x宽x通道数）的格式。方便我们在维度1的连接。

```python
def flatten_pred(pred):
    return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)


def concat_preds(preds):
    return torch.cat([flatten_pred(pred) for pred in preds], dim=1)

```

这样一来，尽管Y1和Y2在通道数，高，宽方面具有不同的大小，但是我们仍然可以在同一个小批量的2个不同尺度上连接这两个预测输出。

```python
>> print(concat_preds([Y1, Y2]).shape)
torch.Size([2, 25300])
```

#### 高和宽减半块

为了在多尺度下检测目标，我们在下面定义了高和宽减半块`down_sample_blk`，该模块将输入特征图的高度和宽度减半。事实上，该块应用了在VGG的模块设计。更具体的说，每个高宽减半块由2个填充为1的3x3的卷积层、以及步幅为2的2x2最大汇聚层（池化层）组成。我们知道。填充为1的3x3卷积层不改变特征图的形状。但是，其后的2x2最大汇聚层将输入特征图的高和宽减半。对于此高宽减半块的输入和输出特征图，因为$1 \times 2 + (3-1) + (3-1) = 6$，所以输出中的每个单元在输入上都有一个6x6的感受野。因此，高和宽减半块会扩大每个单元在其输出特征图中的感受野。

```python
def down_sample_blk(in_channels, out_channels):
    blk = []
    for _ in range(2):
        blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
        blk.append(nn.BatchNorm2d(out_channels))
        blk.append(nn.ReLU())
        in_channels = out_channels
    blk.append(nn.MaxPool2d(2))
    return nn.Sequential(*blk)
```

下面的示例中,我们构建的高宽减半块会更改输入通道的数量,并将输入特征图的高度和宽度减半.

```python
features = forward(torch.zeros((2,3,20,20)), down_sample_blk(3, 10))
print(features.shape)
```

output:

```
torch.Size([2, 10, 10, 10])
```



#### 基本网络块

基本网络块用于从输入图像中抽取特征。为了计算简洁，我们构造一个小的基础网络，该网络串联3个高宽减半块，并逐步将通道数翻倍。给定输入图像的形状为256 * 256，此基本网络块输出的特征图为32 * 32（256/2^3^）

```python
features = forward(torch.zeros((2,3,256,256)), base_net())
print(features.shape)
```

output:

```
torch.Size([2, 64, 32, 32])
```



#### 完整的模型

完整的单发多框检测模型由5个模块组成，每个块生成的特征图即用于生成锚框，又用于预测这些锚框的类别和偏移量。在这五个模块中，第一个是基本网络块，第二个和第五个是高宽减半块。最后一个模块使用全局最大池将宽度和高度都降到1。从技术上讲，第二到第五个区块都是本章开头的图中的"多尺度特征块"。

```python
def get_blk(i):
    if i == 0:
        blk = base_net()
    elif i == 1:
        blk = down_sample_blk(64, 128)
    elif i == 4:
        blk = nn.AdaptiveMaxPool2d((1, 1))
    else:
        blk = down_sample_blk(128, 128)
    return blk

```

现在我们为每个块定义前向传播。与图像分类任务不同，此处的输出包括：CNN特征图Y，在当前尺度下根据Y生成的锚框，预测的这些锚框的类别和偏移量。

```python
from Code.CV.anchor_box import MultiBoxPrior

def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):
    Y = blk(X)
    anchors = MultiBoxPrior(Y, sizes=size, ratios=ratio)
    cls_preds = cls_predictor(Y)
    bbox_preds = bbox_predictor(Y)
    return Y, anchors, cls_preds, bbox_preds
```

回想一下，在本节开头的图中，一个较接近顶部的多尺度特征块是用于检测较大目标的，因此需要生成较大的锚框。在上面的前向传播中，在每个多尺度特征块上，我们通过调用`MultiBoxPrior`函数的size参数传递2个比例值的列表。在下面，0.2和1.05之间的区间被均匀分成5个部分，已确定5个模块的在不同尺度下的较小值：0.2,0.37,0.54,0.71,0.88。之后，他们较大的值由$\sqrt{0.2 \times 0.37} = 0.272$，$\sqrt{0.37 \times 0.54}=0.447$等给出。

```python
sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79], [0.88, 0.961]]
ratios = [[1, 2, 0.5]] * 5
num_anchors = len(sizes[0]) + len(ratios[0]) - 1
```

现在，我们可以按照如下方式定义完整的模型TinySSD了

```python
class TinySSD(nn.Module):
    def __init__(self, num_classes, **kwargs):
        super(TinySSD, self).__init__(**kwargs)
        self.num_classes = num_classes
        idx_to_in_channels = [64, 128, 128, 128, 128]
        for i in range(5):
            setattr(self, f'blk_{i}', get_blk(i))
            setattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i], num_anchors, num_classes))
            setattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i], num_anchors))

    def forward(self, X):
        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5
        for i in range(5):
            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],
                                                                     getattr(self, f"cls_{i}"),
                                                                     getattr(self, f"bbox_{i}"))
        anchors = torch.cat(anchors, dim=1)
        cls_preds = concat_preds(cls_preds)
        cls_preds = cls_preds.reshape(cls_preds.shape[0], -1, self.num_classes + 1)
        bbox_preds = concat_preds(bbox_preds)
        return anchors, cls_preds, bbox_preds
```



我们创建一个模型实例,然后使用它对一个256 * 256的小批量图像X执行前向传播。

如本节前面部分所示，第一个模块输出特征图的形状为32 * 32。回想一下，第二个到第四个模块为高和宽减半块，第5个为全局汇聚层。由于以特征图的每个单元为中心有4个锚框生成，因此在所有尺度下，每个图像共生成$(32^2+16^2+8^2+4^2+1) * 4=5444$个锚框。

```python
net = TinySSD(num_classes=1)
X = torch.zeros((32, 3, 256, 256))
anchors, cls_preds, bboxes = net(X)

print("output anchors", anchors.shape)
print("output class preds", cls_preds.shape)
print("output bbox preds", bboxes.shape)
```

Output:

```txt
output anchors torch.Size([1, 5444, 4])
output class preds torch.Size([32, 5444, 2])
output bbox preds torch.Size([32, 21776])
```

### 定义损失函数和评价函数

目标检测有2种类型的损失，第一种有关锚框类别的损失：我们可以简单的复用之前图像分类问题里使用的交叉熵损失来计算；

第二种有关正类锚框偏移量的损失：预测偏移量是一个回归问题。但是，对于这个回归问题，我们这里不使用平方损失，而是使用L1范数损失。即预测值与真实值之差的绝对值。掩码变量bbox_masks令负类锚框和填充锚框不参与损失的计算。最后，我们将锚框类别和偏移量的损失相加，以获得模型的最终损失函数。

```python
cls_loss = nn.CrossEntropyLoss(reduction='none')
bbox_loss = nn.L1Loss(reduction='none')


def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):
    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]
    cls = cls_loss(cls_preds.reshape(-1, num_classes), cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)
    bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks).mean(dim=1)
    return cls + bbox

```

我们可以沿用准确率评价分类结果。由于偏移量使用了L1范数损失，我们使用平均绝对误差来评价边界框的预测结果。这些预测结果是从生成的锚框及其预测偏移量中获得的。

```python
def cls_eval(cls_preds, cls_labels):
    return float((cls_preds.argmax(dim=-1).type(cls_labels.dtype) == cls_labels).sum())


def bbox_eval(bbox_preds, bbox_labels, bbox_masks):
    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())
```



### 训练模型

在训练模型时，我们需要在模型的前向传播过程中生成多尺度锚框(anchors)，并预测其类别(cls_preds)和偏移量(bbox_preds)。然后我们根据标签信息Y为生成的锚框标记类别(cls_labels)和偏移量(bbox_labels)。最后我们根据类别和偏移量的预测和标注值计算损失函数。为了代码简洁，这里没有评价数据集。

```python
import matplotlib
import matplotlib.pyplot as plt
import time
import numpy as np
from Code.Utils.bbox_dataloader import load_data_pikachu
from Code.Utils.bboxes import MultiBoxTarget
matplotlib.use("TkAgg")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
num_epochs = 20
batch_size = 16
net = TinySSD(num_classes=1)
fig, ax = plt.subplots()
ax.set(xlabel="epoch", xlim=[1, num_epochs], xscale="linear", yscale="linear")
net = net.to(device)
trainer = torch.optim.SGD(net.parameters(), lr=0.2, weight_decay=5e-4)

train_iter, test_iter = load_data_pikachu(batch_size)
cls, bbox = [], []
start_time = time.time()
for epoch in range(num_epochs):
    cls_evals = 0
    labels = 0
    bbox_evals = 0
    bboxes = 0

    net.train()
    for _iter in train_iter:
        features, target = _iter["image"], _iter["label"]
        trainer.zero_grad()
        # print(features, target)
        X, Y = features.to(device), target.to(device)
        # 生成多尺度锚框，为每个锚框预测类别和偏移量
        anchors, cls_preds, bbox_preds = net(X)
        # 为每个锚框标注类别和偏移量
        bbox_labels, bbox_masks, cls_labels = MultiBoxTarget(anchors, Y)
        # 根据类别和偏移量的预测和标注值计算损失函数
        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks)
        l.mean().backward()
        trainer.step()

        cls_evals += cls_eval(cls_preds, cls_labels)
        labels += cls_labels.numel()
        bbox_evals += bbox_eval(bbox_preds, bbox_labels, bbox_masks)
        bboxes += bbox_labels.numel()

        cls_error, bbox_mae = 1 - cls_evals / labels, bbox_evals / bboxes
        cls.append(cls_error)
        bbox.append(bbox_mae)
        print(f"class error: {cls_error:.2e}, bbox mae: {bbox_mae:.2e}")
        print(f"{len(train_iter.dataset) / (time.time()-start_time):.1f} examples/sec on {str(device)}")
        ax.plot(np.arange(0, 20, 1), np.array(cls), label="class error")
        ax.plot(np.arange(0, 20, 1), np.array(bbox), label="bbox mae")
        plt.show()
```

```
class error: 3.75e-04, bbox mae: 2.45e-04
0.9 examples/sec on cuda
```



![image-20220612204609832](src/08.计算机视觉(CV)/image-20220612204609832.png)

### 预测目标

在预测阶段，我们希望能把图像里面所有感兴趣的目标全部检测出来。在下面，我们读取并调整测试图像的大小，然后将其转成卷积层需要的四维格式。

```python
import torchvision
X = torchvision.io.read_image(img_path).unsqueeze(0).float()
img = X.squeeze(0).permute(1, 2, 0).long()
```

使用下面的`MultiBoxDetection`函数，我们可以根据锚框及其预测偏移量得到预测边界框，然后通过非极大值抑制来移除相似的预测边界框。

```python
def predict(X_):
    net.eval()
    anchors, cls_preds, bbox_preds = net(X_.to(device))
    cls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)
    output = MultiBoxDetection(cls_probs, bbox_preds, anchors)
    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]
    return output[0][idx]
output = predict(X)
```

最后，我们筛选置信度不低于0.9的锚框，作为最终输出。

```python
    def display(img, output, threshold):
        import matplotlib.pyplot as plt
        import matplotlib
        matplotlib.use("TkAgg")
        fig = plt.imshow(img)
        for row in output:
            score = float(row[1])
            if score < threshold:
                continue
            h, w = img.shape[0:2]
            bbox = [row[2:6] * torch.tensor([w, h, w, h], dtype=torch.float32, device=device)]
            show_bboxes(fig.axes, bbox, '%.8f' % score, 'w')
        plt.show()
    display(img, output, 0.9)

```

对不起，这一章我复现不出来。



## 八、区域卷积神经网络（R-CNN）系列

区域卷积神经网络(Region-based CNN)，是将深度模型应用于目标检测的开创性工作之一。本节中，我们将介绍R-CNN和他的一系列改进方法(Fast R-CNN, Faster R-CNN, Mask R-CNN)。限于篇幅，这里只介绍这些模型的设计思路。

### R-CNN

R-CNN首先对图像选取若干提议区域（如锚框也是一种选取方法）并标注他们的类别和边界框（如偏移量）。然后，用卷积神经网络对每个提议区域做前向计算抽取特征。之后，我们用每个提议区域的特征预测类别和边界框。下图描绘了R-CNN模型：

![image-20220616195759918](src/08.计算机视觉(CV)/image-20220616195759918.png)

具体来说,R-CNN主要由以下4个步骤构成:

1. 对输入图像使用选择性搜索（selective search）来选取多个高质量提议区域。这些区域通常在多个尺度下选取，并具有不同的形状和大小。每个提议区域将被标注类别和真实边界框。
2. 选取一个预训练的卷积神经网络，并将其在输出层之前截断。将每个提议区域变形为网络需要的输入尺寸。并通过前向计算输出抽取的提议区域特征。
3. 将每个提议区域的特征连同其标注的类别作为一个样本，训练多个支持向量机对目标分类。其中每个支持向量机用来判断样本是否属于某个类别。
4. 将每个提议区域的特征连同其标注的边界框为样本，训练线性回归来预测真实边界框。

R-CNN虽然通过预训练的卷积神经网络有效抽取了图像特征，但它的主要缺点是速度慢。想象一下，我们可能从一张图片选取上千个提议区域，对该图像做目标检测将导致上千次的CNN前向计算。这个巨大的计算量令R-CNN难以在实际应用中被广泛使用。



### Fast R-CNN

R-CNN的主要性能瓶颈在于需要对每个提议区域独立抽取特征。由于这些区域通常会有大量重叠，独立的特征抽取会导致大量的重复计算。Fast R-CNN对R-CNN的主要改进在于只对整个图像做卷积神经网络的前向计算。

Fast R-CNN模型：

![image-20220616203501141](src/08.计算机视觉(CV)/image-20220616203501141-16553829019411.png)

它的主要计算步骤：

1. 与R-CNN相比，Fast R-CNN用于提取特征的卷积神经网络输入是整个图像，而不是各个提议区域。而且，这个网络通常会参与训练，即更新模型参数。设输入为一张图像，将卷积神经网络的输出的形状记为$(1\times c \times h_1 \times w_1)$

2. 假设选择性搜索生成n个提议区域，这些形状各异的提议区域在卷积神经网络的输出上分别标出形状各异的兴趣区域。这些兴趣区域需要抽取出形状相同的特征（假设高宽分别指定h~2~, w~2~）以便连接后输出。Fast R-CNN引入兴趣区域池化（Region of Interest Pooling， ROI池化）层。将卷积神经网络的输出和提议区域作为输入，输出连接后的各个提议区域抽取的特征，形状为$n \times c \times h_2 \times w_2$
3. 通过全连接层将输出形状变换为$n \times d$，其中超参数d取决于模型设计。
4. 预测类别时，将全连接层的输出形状变换为$n \times q$并使用softmax回归(q为类别个数)。预测边界框时，将全连接层的输出形状变换为$n \times 4$。也就是说，我们为每个提议区域预测类别和边界框。

Fast R-CNN中提出的兴趣区域池化层和普通池化层有点不同，在池化层中，我们通过设置池化窗口、填充和步幅来控制输出形状。而兴趣区域池化层对每个区域的输出形状是可以直接指定的。例如，指定每个区域输出的高和宽分别为$h_2,w_2$，假设某一区域窗口的高和宽分别为h和w，该窗口将被划分为形状为$h_2 \times w_2$的子窗口网格，且每个子窗口的大小约为$(h/h_2) \times (w/w_2)$。任意子窗口的高和宽要取整，其中的最大元素作为该子窗口的输出。因此，兴趣区域池化层可从形状各异的兴趣区域中均抽取出形状相同的特征。

如下图，我们在$4 \times 4$的输入上选取了左上角$3 \times 3$区域作为兴趣区域。对于该兴趣区域，我们通过$2 \times 2$兴趣区域池化层得到一个$2 \times 2$的输出。4个划分后的子窗口分别含有元素0，1，4，5（5最大），2，6（6最大），8，9（9最大），10

![image-20220616215302503](src/08.计算机视觉(CV)/image-20220616215302503.png)

​																									兴趣区域池化层

我们使用`ROIPooling`函数来演示兴趣区域池化层的计算.假设卷积神经网络抽取的特征`X`的高和宽均为4且只有单通道。

```python
import torch

X = torch.arange(16, dtype=torch.float).view(1,1,4,4)
print(X)
```

```
tensor([[[[ 0,  1,  2,  3],
          [ 4,  5,  6,  7],
          [ 8,  9, 10, 11],
          [12, 13, 14, 15]]]])
```

假设图像的高和宽为40像素,再假设选择性搜索在图像上生成了2个提议区域:每个区域由5个元素表示，分别为区域目标类别，左上角xy坐标以及右下角xy坐标。

```python
rois = torch.tensor([[0,0,0,20,20], [0,0,10,30,30]], dtype=torch.float)
```

由于`X`的高和宽是图像高和宽的1/10，以上2个提议区域中的坐标先按`spatial_scale`自乘0.1，然后在`X`上分别标出兴趣区域`X[:,:,0:3,0:3]`和`X[:,:,1:4,0:4]`。最后对这两个兴趣区域分别划分子窗口并抽取高和宽为2的特征。

```python
print(torchvision.ops.roi_pool(X, rois, output_size=(2, 2), spatial_scale=0.1))
```

```
tensor([[[[ 5.,  6.],
          [ 9., 10.]]],


        [[[ 9., 11.],
          [13., 15.]]]])
```



### Faster R-CNN

Fast R-CNN通常在选择性搜索中生成较多的提议区域，以获得较精确的目标检测结果。Faster-RCNN提出将选择性搜索替换成区域提议网络（region proposal network），从而减少提议区域的生成数量，并保证目标检测的精度。

![image-20220618203357774](src/08.计算机视觉(CV)/image-20220618203357774.png)

图9.8描述了Faster R-CNN模型，与Fast R-CNN相比，只有生成提议区域的方法从选择性搜索变成了区域提议网络，而其他部分均保持不变。具体来说，区域提议网络的计算步骤如下：

1. 使用填充为1的$3 \times 3$卷积层变换卷积神经网络的输出。并将输出通道标记为c。这样，卷积神经网络为图像抽取的特征图中每个单元均得到一个长度为c的新特征。
2. 以特征图的每个单元为中心，生成多个不同大小和宽高比的锚框并标注他们。
3. 用锚框中心单元长度为c的特征分别预测该锚框的二元类别（目标还是背景）和边界框。

4. 使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即兴趣区域池化层所需要的提议区域。

值得一提的是，区域建议网络作为Faster R-CNN的一部分，是和整个模型一起训练得到的。也就是说，Faster R-CNN目标函数既包括目标检测中的类别和边界框预测，也包括区域提议网络中锚框的二元类别和边界框预测。最终，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少提议区域数量的情况下也能保证目标检测的精度。



### Mask R-CNN

如果训练数据还标注了每个目标在图像上的像素级位置，那么Mask R-CNN能有效利用这些详尽的标注信息进一步提升目标检测精度。

![image-20220619203958383](src/08.计算机视觉(CV)/image-20220619203958383.png)

如图，Mask R-CNN在Faster R-CNN基础上作了修改。Mask R-CNN将兴趣区域池化层替换成了兴趣区域对齐层，即通过双线性插值来保留特征图上的空间信息，从而更适用于像素级预测。兴趣区域对齐层的输出包含了所有兴趣区域的形状相同的特征图。他们既用来预测兴趣区域的类别和边界框，又通过额外的全卷积网络预测目标的像素级位置。我们将在全卷积网络章节中介绍如何使用全卷积网络预测图像中像素级的语义。



## 九、语义分割和数据集

在前几节讨论的目标检测问题中，我们一直使用方形边界框来标注和预测图像中的目标。本节将讨论语义分割（semantic segmentation）问题。它关注于如何将图像分割成属于不同语义类别的区域。值得一提的是，这些语义区域的标注和预测都是像素级的。下图展示了语义分割中图像有关狗，猫，背景的标签。可以看到，与目标检测相比，语义分割标注的像素边框更加精细。

![image-20220619213656516](src/08.计算机视觉(CV)/image-20220619213656516.png)

### 图像分割和实例分割

计算机视觉领域还有2个与语义分割相似的重要问题，即图像分割（image segmentation）和实例分割（instance segmentation）。在这里我们将他们同语义分割区分一下：

* 图像分割将图像区分成若干组成区域。这类问题的方法通常利用图像像素之间的相关性。他在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割的区域具有我们希望的语义。图像分割可能将狗分割成2个区域：一个覆盖以黑色为主的嘴巴和眼睛，另一个覆盖以黄色为主的身体区域。
* 实例分割又叫同时检测并分割（semutaneous detection and segmentation）。他研究如何识别图像中不同实例的像素级区域。与语义分割有所不同，实例分割不仅需要分割语义，还要区分不同的目标实例。如果图像中有2只狗，实例分割需要区分像素属于这两只狗的哪一只。



### Pascal VOC2012 语义分割数据集

语义分割的一个重要数据集叫Pascal VOC2012。为了更好地了解这个数据集，我们先导入实验所需要的包和模块。

```python
import urllib3
import os
import tqdm
from PIL import Image
from Code.Utils.image_augmentation import show_images, plt
```

我们先下载数据集压缩包（[下载地址](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar)），大小为2G左右，下载需要一定时间，下载后解压得到`VOCdevkit/VOC2012`目录，将其放在datasets下

备用下载地址：[下载地址](http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar)

```bash
ls VOCtrainval_11-May-2012/VOCdevkit/VOC2012
```

Output:

```
Annotations  ImageSets  JPEGImages  SegmentationClass  SegmentationObject
```

进入`VOCtrainval_11-May-2012/VOCdevkit/VOC2012`路径后,我们可以获取数据集的不同组成部分,其中`ImageSets/Segmentation`包含了指定训练和测试的文本文件，而`JPEGImages`和`SegmentationClass`路径下分别包含了样本的输入图像和标签。这里的标签也是图像格式，其尺寸和它输入图像的尺寸相同。标签中颜色相同的像素属于同一个类别。下面定义`read_voc_images`函数将输入图像和标签读进内存。

```python
def read_voc_images(root, is_train, max_num=None):
    root = os.path.join(root, "VOCdevkit", "VOC2012")
    txt_fname = os.path.join(root, "ImageSets", "Segmentation", "train.txt" if is_train else "val.txt")
    with open(txt_fname) as f:
        images = f.read().split()
    if max_num is not None:
        images = images[:min(max_num, len(images))]
    features, labels = [None] * len(images), [None] * len(images)
    for i, fname in tqdm.tqdm(enumerate(images)):
        features[i] = Image.open(os.path.join(root, "JPEGImages", fname+".jpg")).convert("RGB")
        labels[i] = Image.open(os.path.join(root, "SegmentationClass", fname+".png")).convert("RGB")
    return features, labels
```

```python
n = 5
root = "/home/zhaozijian/Downloads/VOCtrainval_11-May-2012"
features, labels = read_voc_images(root, True, 100)
show_images(features[0:n]+labels[0:n], 2, n)
plt.show()
```



![image-20220621001952246](src/08.计算机视觉(CV)/image-20220621001952246.png)

接下来,我们列出标签中每个RGB颜色的值和标注的类别。

```python
VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],
                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],
                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],
                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],
                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],
                [0, 64, 128]]

VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',
               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
               'diningtable', 'dog', 'horse', 'motorbike', 'person',
               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']

```

有了上面2个常量之后，我们可以很容易的查找标签中每个像素的类别索引。

```python
def voc_label_indices(colormap, colormap2label):
    colormap = np.array(colormap.convert("RGB")).astype("int32")
    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256 + colormap[:, :, 2])
    return colormap2label[idx]

y = voc_label_indices(labels[0], colormap2label)
print(y[105: 115, 130: 140], VOC_CLASSES[1])
```

Output：

```
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]], dtype=torch.uint8) aeroplane
```



### 预处理数据

在之前的章节中，我们通过缩放图像使其符合模型的输入图像。然而在语义分割里，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。这样做难以做到精确，尤其是在不同语义的分割区域。为了避免这个问题，我们将图像裁剪成固定区域而不是缩放。具体来说，我们使用图像增广里的随机裁剪，并对输入图像和标签裁剪相同区域。

```python
def read_voc_images(root, is_train, max_num=None):
    root = os.path.join(root, "VOCdevkit", "VOC2012")
    txt_fname = os.path.join(root, "ImageSets", "Segmentation", "train.txt" if is_train else "val.txt")
    with open(txt_fname) as f:
        images = f.read().split()
    if max_num is not None:
        images = images[:min(max_num, len(images))]
    features, labels = [None] * len(images), [None] * len(images)
    for i, fname in tqdm.tqdm(enumerate(images)):
        features[i] = Image.open(os.path.join(root, "JPEGImages", fname+".jpg")).convert("RGB")
        labels[i] = Image.open(os.path.join(root, "SegmentationClass", fname+".png")).convert("RGB")
    return features, labels

```

```python
imgs = []
for _ in range(n):
	imgs += voc_rand_crop(features[0], labels[0], 200, 300)
show_images(imgs[::2] + imgs[1::2], 2, 5)
plt.show()
```

![image-20220621234334262](src/08.计算机视觉(CV)/image-20220621234334262.png)

### 自定义语义分割数据集类

我们通过继承pytorch的`Dataset`类自定义了一个语义分割数据集`VOCSegDataset`。通过实现`__getitem__`函数，我们可以访问任意数据集中索引为`idx`的输入图像以及每个像素的类别索引。由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本需要通过自定义的`filter`函数移除。此外，我们还对输入图像的rgb三通道值做标准化。

```python
class VOCSegDataset(torch.utils.data.Dataset):
    def __init__(self, is_train, crop_size, voc_dir, colormap2label, max_num=None):
        # https://blog.csdn.net/qq_41076797/article/details/111005890
        self.rgb_mean = np.array([0.485, 0.456, 0.406])
        self.rgb_std = np.array([0.229, 0.224, 0.225])
        self.tsf = torchvision.transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize(mean=self.rgb_mean, std=self.rgb_std)
        ])
        self.crop_size = crop_size
        features, labels = read_voc_images(root=voc_dir, is_train=is_train, max_num=max_num)
        self.features = self.filter(features)
        self.labels = self.filter(labels)
        self.colormap2label = colormap2label
        print("read " + str(len(features)) + " vaild examples")

    def filter(self, imgs):
        return [img for img in imgs if (
            img.size[1] >= self.crop_size[0] and
            img.size[0] >= self.crop_size[1]
        )]

    def __getitem__(self, idx):
        feature, label = voc_rand_crop(self.features[idx], self.labels[idx], *self.crop_size)
        return self.tsf(feature), voc_label_indices(label, self.colormap2label)

    def __len__(self):
        return len(self.features)
```



### 读取数据集

我们通过自定义的`VOCSegDataset`类来分别创建训练集和数据集的实例，假设我们指定随机裁剪的输出图像形状为$320 \times 480$。下面我们可以查看训练集和测试集所保留的样本数。

```python
crop_size = (320, 480)
max_num = 100
voc_train = VOCSegDataset(True, crop_size, root, colormap2label, max_num)
voc_test = VOCSegDataset(False, crop_size, root, colormap2label, max_num)
```

```
read 100 vaild examples
read 100 vaild examples
```

设批量大小为64,分别定义训练集和数据集的迭代器.

```python
batch_size = 64
num_worker = 0 if sys.platform.startswith("win32") else 4
train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True,
                                         drop_last=True, num_workers=num_worker)
test_iter = torch.utils.data.DataLoader(voc_test, batch_size, drop_last=True, num_workers=num_worker)
    
```

打印第一个小批量的类型和形状。不同于图像分类和目标识别，这里的标签是一个三维数组

```python
for X, y in train_iter:
    print(X.shape, X.dtype)
    print(y.shape, y.dtype)
    break
```

```
torch.Size([64, 3, 320, 480]) torch.float32
torch.Size([64, 320, 480]) torch.uint8
```





## 十、全卷积网络

待完成。。





## 十一、样式迁移

如果你是一位摄影爱好者，也许接触过滤镜，它能改变图片的样式，从而使风景更加锐利或美白。但一个滤镜通常只能改变图像的某个方面。如果要达到理想中的样式，经常需要尝试大量不同的场合，其复杂程度不亚于模型调参。

在本节中，我们将介绍如何使用卷积神经网络自动将图片中的样式应用于另一图片之上，即为样式迁移(style transfer)，我们需要准备2张输入图像，一张是内容图像，另一张是样式图像，我们将使用神经网络修改内容图像使其在样式上接近样式图像。下图中内容图像为本书作者拍摄的风景照，而样式图像则是一副秋天主题的油画，最终输出的合成图像在保留了内容图像中物体的主体形状的情况下修改为了油画风格，色彩更加鲜艳。

![image-20220709134145243](src/08.计算机视觉(CV)/image-20220709134145243.png)

### 方法

首先我们初始化合成图像，例如将其初始化为内容图像，该合成图像是样式迁移过程中唯一需要更新的变量，即样式迁移所需迭代的模型参数。然后，我们选择一个预训练的卷积神经网络来抽取图像特征，其中的模型参数在训练中无需更新。深度卷积神经网络逐层抽取的图像特征。我们可以选择其中某个层的输出作为内容特征或者样式特征。以上图为例，这里选取的预训练的神经网络具有3个卷积层，其中第二层输出图像的内容特征，第一层和第三层输出图像的样式特征，接下来我们通过正向传播计算样式迁移的损失函数。并通过反向传播迭代模型参数，即不断更新合成图像，样式迁移常用的损失函数由三部分组成：**内容损失**（content loss）使合成图像与内容图像在内容特征上接近。**样式损失**（style loss）令合成图像和样式图像在样式特征上接近，而**总变差损失**（total variation loss）则有助于减少图像中的噪点。最后，当模型训练结束后，我们输出样式迁移的模型参数，即得到最终的合成图像。

![image-20220709144331464](src/08.计算机视觉(CV)/image-20220709144331464.png)

下面我们通过实验来进一步了解样式迁移的细节。

* 先导入

```python
import time
import torch
import torch.nn.functional as F
import torchvision
import numpy as np
import os
import matplotlib.pyplot as plt
import matplotlib

from PIL import Image
import sys
from Code import DATADIR, MODELDIR

matplotlib.use("TkAgg")
```

### 读取内容图像和样式图像

首先,我们分别读取内容图像和样式图像。从打印出的图像坐标轴可以看出他们的尺寸并不一样。

```python
content_img = Image.open(os.path.join(DATADIR, "style_transfer", "rainier.jpg"))
plt.imshow(content_img)
plt.show()
```



![image-20220709150323689](src/08.计算机视觉(CV)/image-20220709150323689.png)

```python
style_img = Image.open(os.path.join(DATADIR, "style_transfer", "autumn_oak.jpg"))
plt.imshow(style_img)
plt.show()
```

![image-20220709150508876](src/08.计算机视觉(CV)/image-20220709150508876.png)

### 预处理和后处理图像

下面定义图像的预处理和后处理函数。预处理函数`preprocess`更改图像输入尺寸，然后再将PIL图片转成卷积神经网络接受的输入格式，再在RGB三个通道分别作标准化，由于预训练模型是在均值为[0.485, 0.456, 0.406]，标准差为[0.229, 0.224, 0.225]的图片数据上预训练的，所以我们需要将图片保持相同的均值和标准差。后处理函数postprocess则将输出图像中的像素值还原回标准化之前的值。由于图像中每个像素浮点数在0-1之间，我们使用`clamp`函数对小于0和大于1的值分别取0和1.

> torchvision.transformer中包含大量的转换器，但是有些转换器的输入是PIL图像，如Resize，有些是tensor，如Normalize；而有的用于二者转换，如Totensor，使用前一定要看清文档。

```python
rgb_mean = np.array([0.485, 0.456, 0.406])
rgb_std = np.array([0.229, 0.224, 0.225])


def preprocess(pil_image, image_shape):
    process = torchvision.transforms.Compose([
        torchvision.transforms.Resize(image_shape),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)
    ])
    return process(pil_image).unsqueeze(0)


def postprocess(img_tensor):
    inv_normalize = torchvision.transforms.Normalize(
        mean=-rgb_mean / rgb_std,
        std=1/rgb_std,
    )
    to_PIL_image = torchvision.transforms.ToPILImage()
    return to_PIL_image(inv_normalize(img_tensor[0].cpu()).clamp(0, 1))

```



### 抽取特征

我们使用基于ImageNet数据集预训练的VGG-19模型来抽取图像特征.

> pytorch官方在torchvision.models模块提供了一些常见的预训练好的计算机视觉模型，包括图片分类，语义分割，目标检测，实例分割，人关键点检测和视频分类等。使用时需仔细阅读文档，搞清楚如何使用。

```python
# pytorch会将模型下载到TORCH_MODEL环境变量目录下，否则将会下载到.cache/torch下
os.environ["TORCH_HOME"] = MODELDIR
pretrianed_net = torchvision.models.vgg19(pretrained=True, progress=True)
```

第一次执行时会自动下载model。

为了抽取图像内容特征和样式特征，我们可以选择VGG网络中的某一层进行输出。一般来说，越靠近输入层的输出越容易抽取图像的细节信息，反之则越容易抽取图像的全局信息。为了避免合成图像过多保留内容图像的细节，我们选择VGG较靠近输出的层，也成内容层，来输出图像的内容特征。我们还从VGG中选择不同的层输出来匹配局部和全局的样式，这些曾叫样式层，在VGG中我们介绍过，VGG网络使用了5个卷积块。实验中我们选择第四卷积块的最后一个卷积层作为内容层，以及每个卷积块的第一个卷积层作为样式层。这些层的索引可以通过打印pretrained_net实例来获取。

```python
print(pretrained_net)
```

```
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): ReLU(inplace=True)
    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): ReLU(inplace=True)
    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): ReLU(inplace=True)
    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (33): ReLU(inplace=True)
    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): ReLU(inplace=True)
    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
```

在抽取特征时，我们只需要用到VGG从输入层到最靠近输出层的内容层或样式层之间的所有层，下面构成一个新的网络，他只保留需要用到的VGG的所有层，我们使用`net`来抽取特征。

```python
style_layers = [0, 5, 10, 19, 28]
content_layers = [25]
net_list = []
for i in range(max(content_layers + style_layers) + 1):
    net_list.append(pretrianed_net.features[i])
net = torch.nn.Sequential(*net_list)
```

给定输入`X`，如果简单调用全向计算`net(X)`，只能获得最后一层的输出,由于我们还需要中间层的输出,因此我们这里逐层计算,并保留内容层的样式和输出。

```python
def extract_features(X, content_layers, style_layers):
    contents = []
    styles = []
    for i in range(len(net)):
        X = net[i](X)
        if i in style_layers:
            styles.append(X)
        if i in content_layers:
            contents.append(X)
    return contents, styles
```

下面定义2个函数,其中`get_contents`函数对内容图像抽取内容特征，而`get_style`函数则对样式图像抽取样式特征。因为在训练时无需改变预训练的VGG模型参数，所以我们可以在预训练之前就提取出内容图像的内容特征，以及样式图像的样式特征。由于合成图像是样式迁移所需迭代的模型参数，我们只能通过训练时调用`extract_features`函数来抽取合成图像的内容特征和样式特征。

```python
def get_contents(image_shape, device):
    content_X = preprocess(content_img, image_shape).to(device)
    content_Y, _ = extract_features(content_X, content_layers, style_layers)
    return content_X, content_Y


def get_styles(image_shape, device):
    style_X = preprocess(style_img, image_shape).to(device)
    _, style_Y = extract_features(style_X, content_layers, style_layers)
    return style_X, style_Y

```



### 定义损失函数

下面我们定义内容损失,样式损失和总变差损失

#### 内容损失

与线性回归中的损失函数类似，内容损失平方误差函数衡量合成图像与内容图像在内容特征上的差异。

平方误差函数2个输入均为`extract_features`函数计算所得到的内容层的输出。

```python
def content_loss(Y_hat, Y):
    return F.mse_loss(Y_hat, Y)

```

#### 样式损失

样式损失也一样通过平方误差函数衡量合成图像与样式图像在样式上的差异。为了表达样式层输出的样式我们先通过`extract_features`函数计算样式层的输出。假设该输出的样本数为1，通道数为c，高和宽分别为h,w。我们可以把输出变为$c$行$hw$列的矩阵$X$。矩阵$X$可以看做是由c个长度为hw的向量$x_1...x_c$组成，其中向量$x_i$代表了通道i上的样式特征。这些向量的格拉姆矩阵$XX^\top \in \mathbb{R}^{c \times c}$中i行j列的元素$x_{ij}$,即向量$x_i$和$x_j$的内积。他表达了通道i和通道j上样式特征的相关性。我们用这样的格拉姆矩阵表达样式层输出的样式。需要注意的是，当hw的值较大时，格拉姆矩阵中的元素容易出现较大的值。此外，格拉姆矩阵的高和宽皆为通道数c。为了让损失函数不受这些值大小影响，下面定义`gram`函数将格拉姆矩阵除以矩阵中元素个数，即$chw$

```python
def gram(X):
    num_channels, n = X.shape[1], X.shape[2] * X.shape[3]
    X = X.view(num_channels, n)
    return torch.matmul(X, X.t()) / (num_channels * n)
```

自然地,样式损失的平方误差函数的两个格拉姆矩阵输入分别基于合成图像和样式图像的样式层输出，这里假设基于样式图像的格拉姆矩阵`gram_Y`已经计算好了。

```python
def style_loss(Y_hat, gram_Y):
    return F.mse_loss(gram(Y_hat), gram_Y)
```



#### 总变差损失

有时候，我们学到的合成图像中有大量的高频噪点，即有特别亮或者特别暗的颗粒像素，一种常用的降噪方法是总变差降噪(total variation denoising)。假设$x_{i,j}$表示坐标为$(i,j)$的像素值，降低总变差损失

​																					$\sum_{i,j}|x_{i,j}-x_{i+1,j}| + |x_{i,j}-x_{i,j+1}|$

能够尽可能使邻近的像素值相似。

```python
def vt_loss(Y_hat):
    return 0.5 * (F.l1_loss(Y_hat[:, :, 1:, :], Y_hat[:, :, :-1, :])) + \
                    F.l1_loss(Y_hat[:, :, :, 1:], Y_hat[:, :, :, :-1])
```



#### 损失函数

样式迁移的损失函数即内容损失,样式损失,和总变差损失的加权和。通过调节这些权值超参，我们可以衡量合成图像在保留内容，迁移样式以及降噪三方面的相对重要性。

```python

def compute_loss(X, content_Y_hat, styles_Y_hat, content_Y, style_Y_gram):
    """分别计算内容损失，样式损失，总变差损失"""
    content_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(content_Y_hat, content_Y)]
    style_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(styles_Y_hat, style_Y_gram)]
    tv_l = tv_loss(X) * tv_weight
    # 对所有损失求和
    l = sum(style_l) + sum(content_l) + tv_l
    return content_l, style_l, tv_l, l

```



#### 创建和初始化合成图像

在样式迁移中，合成图像是唯一需要更新的变量。因此我们可以定义一个简单的模型`GeneratedImage`并将合成图像视为模型参数，模型的前向计算只需返回模型参数即可。

````python
class GeneratedImage(torch.nn.Module):
    def __init__(self, image_shape):
        super(GeneratedImage, self).__init__()
        self.weight = torch.nn.Parameter(torch.rand(*image_shape))

    def forward(self):
        return self.weight
````

下面我们定义一个`get_inits`函数，该函数创建了合成图像的模型实例，并将其初始化为图像`X`，样式图像在各个样式层的格拉姆矩阵`styles_Y_gram`将在训练前预先计算好。

```python
def get_inits(X, device, lr, styles_Y):
    gen_img = GeneratedImage(X.shape).to(device)
    gen_img.weight.data = X.data
    optimizer = torch.optim.Adam(gen_img.parameters(), lr=lr)
    styles_Y_gram = [gram(Y) for Y in styles_Y]
    return gen_img(), styles_Y_gram, optimizer
```



#### 训练

训练模型时，我们不断的抽取合成图像的内容特征和样式特征，并计算损失函数

```python
def train_st(X, content_Y, styles_Y, device, lr, max_epochs, lr_decay_epoch):
    print("training on %s" % device)
    X, styles_Y_gram, optimizer = get_inits(X, device, lr, styles_Y)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_decay_epoch, gamma=0.1)
    for i in range(max_epochs):
        start = time.time()

        contents_Y_hat, styles_Y_hat = extract_features(X, content_layers, style_layers)
        contents_l, styles_l, tv_l, l = compute_loss(X, contents_Y_hat, styles_Y_hat, content_Y, styles_Y_gram)
        optimizer.zero_grad()
        l.backward(retain_graph = True)
        optimizer.step()
        scheduler.step()
        if i % 50 == 0 and i != 0:
            print("epoch %d, content loss %.2f, style loss %.2f, TV loss %.2f, %.2f sec" % \
                  (i, sum(contents_l).item(), sum(styles_l).item(), tv_l.item(), time.time()-start))
    return X.detach()

```

下面我们开始训练模型，首先将内容图像和样式图像的高和宽分别调整为150和225像素。合成图像将由内容图像来初始化。

```python
image_shape = (150, 225)
net = net.to(device)
content_X, content_Y = get_contents(image_shape, device)
style_X, style_Y = get_styles(image_shape, device)
output = train_st(content_X, content_Y, style_Y, device, lr=0.01, max_epochs=500, lr_decay_epoch=200)
plt.imshow(postprocess(output))
plt.show()
```

outputs：

```
training on cuda
epoch 50, content loss 0.24, style loss 1.26, TV loss 1.64, 0.47 sec
epoch 100, content loss 0.24, style loss 0.92, TV loss 1.48, 0.47 sec
epoch 150, content loss 0.24, style loss 0.83, TV loss 1.37, 0.47 sec
epoch 200, content loss 0.24, style loss 0.78, TV loss 1.30, 0.47 sec
epoch 250, content loss 0.24, style loss 0.77, TV loss 1.28, 0.47 sec
epoch 300, content loss 0.24, style loss 0.77, TV loss 1.28, 0.47 sec
epoch 350, content loss 0.24, style loss 0.77, TV loss 1.27, 0.47 sec
epoch 400, content loss 0.24, style loss 0.76, TV loss 1.26, 0.47 sec
epoch 450, content loss 0.24, style loss 0.76, TV loss 1.26, 0.49 sec
```



![image-20220717144326915](src/08.计算机视觉(CV)/image-20220717144326915.png)

以上代码被整合到`Code/CV/Style_transfer.py`中

