# 循环神经网络

## 语言模型

语言模型是自然语言处理的重要技术，这其中最常见的数据是文本数据，我们可以把自然语言文本看做一段离散的时间序列

假设一段长度为T的文本中的词依次为w1, w2, w3....w~t~, 那么在离散的时间序列中，w~t~可看做在时间部t的输出或者标签，给定一个长度为T的词序列w1, w2, w3....w~t~，语言模型将计算该序列的概率：

![image-20210927213507458](src/04.循环神经网络/image-20210927213507458.png)

语言模型可以作为机器翻译或者语音识别的功能。例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。



### 语言模型的计算

假设w1, w2....w~t~每个词都是依次生成的，我们有

![image-20210927215050934](src/04.循环神经网络/image-20210927215050934.png)

例如：一个含有4个词的文本序列概率

![image-20210927215507377](src/04.循环神经网络/image-20210927215507377.png)

说明：

我们可以使用一个大型的语言文本库，如维基百科的所有条目，词的概率可以通过改词在训练集的词频（词出现的次数）与总词数之比来计算，根据条件概率，一个词在给定前几个词的情况下条件概率可以计算。例如P(w2|w1)可以理解为w1w2相邻的概率除以w1

### n元语法

当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会成指数级增加。n元模型通过**马尔科夫假设**简化了语言模型的计算。

**马尔科夫假设**指的是假设一个词的出现只与前面n个词相关，假设n=1，那么w3的出现只与w2相关，与w1不相关：P(w3|w1,w2) = P(w3|w2)

基于n-1阶马尔科夫假设链，我们可以将语言模型改写为：

![image-20210927232039087](src/04.循环神经网络/image-20210927232039087.png)

上面的语法也称作n元语法，它是基于n-1阶马尔科夫链，当n分别为1,2,3时，也称为**1元语法，2元语法，3元语法**

![image-20210927232442745](src/04.循环神经网络/image-20210927232442745.png)

当n较小时，往往结果并不准确，例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。当n较大时，n元语法需要存储大量的词频和多词相邻概率。有没有方法在语言模型中更好的平衡以上2点呢？那就是**循环神经网络**。



## 循环神经网络-RNN