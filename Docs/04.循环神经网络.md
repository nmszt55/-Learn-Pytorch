# 循环神经网络

## 语言模型

语言模型是自然语言处理的重要技术，这其中最常见的数据是文本数据，我们可以把自然语言文本看做一段离散的时间序列

假设一段长度为T的文本中的词依次为w1, w2, w3....w~t~, 那么在离散的时间序列中，w~t~可看做在时间部t的输出或者标签，给定一个长度为T的词序列w1, w2, w3....w~t~，语言模型将计算该序列的概率：

![image-20210927213507458](src/04.循环神经网络/image-20210927213507458.png)

语言模型可以作为机器翻译或者语音识别的功能。例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。



### 语言模型的计算

假设w1, w2....w~t~每个词都是依次生成的，我们有

![image-20210927215050934](src/04.循环神经网络/image-20210927215050934.png)

例如：一个含有4个词的文本序列概率

![image-20210927215507377](src/04.循环神经网络/image-20210927215507377.png)

说明：

我们可以使用一个大型的语言文本库，如维基百科的所有条目，词的概率可以通过改词在训练集的词频（词出现的次数）与总词数之比来计算，根据条件概率，一个词在给定前几个词的情况下条件概率可以计算。例如P(w2|w1)可以理解为w1w2相邻的概率除以w1

### n元语法

当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会成指数级增加。n元模型通过**马尔科夫假设**简化了语言模型的计算。

**马尔科夫假设**指的是假设一个词的出现只与前面n个词相关，假设n=1，那么w3的出现只与w2相关，与w1不相关：P(w3|w1,w2) = P(w3|w2)

基于n-1阶马尔科夫假设链，我们可以将语言模型改写为：

![image-20210927232039087](src/04.循环神经网络/image-20210927232039087.png)

上面的语法也称作n元语法，它是基于n-1阶马尔科夫链，当n分别为1,2,3时，也称为**1元语法，2元语法，3元语法**

![image-20210927232442745](src/04.循环神经网络/image-20210927232442745.png)

当n较小时，往往结果并不准确，例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。当n较大时，n元语法需要存储大量的词频和多词相邻概率。有没有方法在语言模型中更好的平衡以上2点呢？那就是**循环神经网络**。



## 循环神经网络-RNN

循环神经网络通过隐藏状态存储之前时间部的信息.类似于之前的多层感知机。

### 不含隐藏状态的神经网络

考虑我们有个单隐藏层的多层感知机，假如激活函数为*ϕ*，那么隐藏层的输出为

![image-20210930145658621](src/04.循环神经网络/image-20210930145658621.png)

隐藏层权重参数W~xh~ ∈ R^nxh^, 偏差函数b ∈ R^1xh^,h为隐藏单元数，经过输出层：

![image-20210930152302222](src/04.循环神经网络/image-20210930152302222.png)

我们通过softmax函数可以进行分类计算



### 含隐藏状态的循环神经网络

假设时间步为t, 我们基于多层感知机，引入一个隐藏状态进行计算：

![image-20210930153835488](src/04.循环神经网络/image-20210930153835488.png)

这个隐藏状态包含了一个新的权重参数W~hh~, 代表在时间步t-1的计算结果，也就是说当前Ht的计算也同时依赖于上一步的计算值，H~t-1~同理依赖H~t-2~的计算结果。那么我们就构造了一个带有隐藏状态的循环神经网路。

最后，我们在加上一层输出层就ok了

![image-20210930164838861](src/04.循环神经网络/image-20210930164838861.png)

**循环神经网络的参数**

循环神经网络的参数包括隐藏层的W~xh~ W~hh~,和偏差b~h~,还包括输出层的W~hq~和B~q~,**即使是在不同的时间步，循环神经网络也依然使用这些参数**，因此它的参数数量不随着时间步的增加而增长。

**循环神经网络的结构**

![image-20211001112234477](src/04.循环神经网络/image-20211001112234477.png)

如图展示了三个时间步内的计算逻辑，在时间步t中，隐藏状态的计算可以看成是将输入与H~t-1~联结后输出到激活函数的全连接层。

**示例**：

```Python
import torch

X, W_xh = torch.randn(3, 1), torch.randn(1, 4)
H, W_hh = torch.randn(3, 4), torch.randn(4, 4)
torch.matmul(X, W_xh) + torch.matmul(H, W_hh)
```

我们的输入形状如果为(3,1), 权重参数形状为(1,4),他们计算出来的结果形状为(3,4)，此时也就限制了H~t-1~的形状和W~hh~的形状。H~t-1~的形状必须为(3,4), W~hh~必须保证H~t-1~与其计算后形状仍是H~t-1~的形状。

![image-20211003000329790](src/04.循环神经网络/image-20211003000329790.png)

优化计算：其实我们将X和H~t-1~在列维度(dim=1)进行相连，形状为(3,1) + (3, 4) = (3,5), 

W_xh和W_hh在行维度相连，形状为(1,4)+(4,4) = (5,4)，

我们将两个组合后的结果进行运算，还是同样得到一个(3,4)的结果，其结果与上面的计算一致

```Python
torch.matmul(torch.cat((X, H), dim=1), torch.cat((W_xh, W_hh), dim=0))
```





## 语言模型数据集

本节将介绍一个处理语言模型的数据集，并将其转换成字符级循环神经网络所需要的输入格式，我们收集了周杰伦的十张专辑的歌词，并在循环神经网络中训练，训练好后，可以使用其来生成歌词。

### 数据集加载

1. **读取数据集**

```Python
def get_lyric(filepath=os.path.join(Root, "Datasets", "jaychou_lyrics.txt.zip")):
    if not os.path.isfile(filepath):
        raise FileNotFoundError("Cant find datasets of jaychou_lyrics")
    with zipfile.ZipFile(filepath) as zip:
        with zip.open("jaychou_lyrics.txt") as f:
            corpus_chars = f.read().decode("utf-8")
            return corpus_chars
```

查看数据集的前四十个字符为：

![image-20211003140450675](src/04.循环神经网络/image-20211003140450675.png)

为了方便训练，我们使用前10000个字符，并将换行符替换成空格。

```Python
corpus_chars = get_lyric()
corpus_chars = corpus_chars.replace('\n', ' ').replace('\r', ' ')
corpus_chars = corpus_chars[0:10000]
```

2. **建立字符索引**

我们将每个字符映射成从0开始的连续整数，又称索引，方便后续的数据处理，我们将其映射到字典，并打印出vocab_size,也就是不同字符的个数。

代码：

```Python
strings = get_lyric_string()[:10000]
string_list = strings.replace('\n', ' ').replace('\r', ' ')
string_list = string_list[0:10000]
dic = get_string_dict(string_list)
vocab_size = len(dic)
indics = [dic[x] for x in strings]
return indics, dic, string_list, vocab_size
```

代码路径封装在Code/Utils/load_data_jay_lyrics.py



### 时序数据的采样

我们训练时需要随机读取小批量样本和标签，但是与之前不同的是，标签通常包含连续的字符，假设时间步为5，样本序列有5个字符组成：“想要有直升”，该样本的标签序列为字符集中的下一个字符，即：“要有直升机”、

我们可以采取随机采样和相邻采样两种方式进行采样。

#### 随机采样

随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的2条数据在原始数据中不一定相邻，我们无法使用小批量的隐藏状态来初始化下一个批量的状态，因此在每次训练新的批次时，都需要清零隐藏状态。

代码：

```Python
def data_iter_random(index_list, batch_size, num_steps, device=None):
    """
    随机获取数据集
    index_list: 字符串转换成index后的列表,函数会从该list中抽取连续的字符片段作为训练的输入
    batch_size: 批量大小
    num_steps: 时间步大小
    device: 数据设备
    """
    num_example = (len(index_list) - 1) // num_steps
    epoch_size = num_example // batch_size
    example_indices = list(range(num_example))
    random.shuffle(example_indices)

    # 返回从pos开始的，长度为num_steps的序列
    def _data(pos):
        return index_list[pos: pos + num_steps]
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    for i in range(epoch_size):
        i = i * batch_size
        batch_indices = example_indices[i: i+batch_size]
        X = [_data(j*num_steps) for j in batch_indices]
        Y = [_data(j*num_steps+1) for j in batch_indices]
        yield (torch.tensor(X, dtype=torch.float32, device=device),
               torch.tensor(Y, dtype=torch.float32, device=device))
```

验证：

```Python
x = list(range(30))
for x, y in data_iter_random(x, 2, 6):
    print(x)
    print(y)
    print("---")
```

打印：

![image-20211003181222183](src/04.循环神经网络/image-20211003181222183.png)



#### 相邻采样

相邻采样使得在几个批次样本之间维度相同的变量相邻，这样做可以使计算时下一次的输入与这一次的隐藏状态输出相关联

代码：

```Python
def data_iter_consecutive(index_list, batch_size, num_steps, device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    index_list = torch.tensor(index_list, dtype=torch.float32, device=device)
    data_len = len(index_list)
    batch_len = data_len // batch_size
    indices = index_list[0: batch_size * batch_len].view((batch_size, batch_len))
    epoch_size = (batch_len - 1) // num_steps
    for i in range(epoch_size):
        i = i * num_steps
        X = indices[:, i:i+num_steps]
        Y = indices[:, i+1: i+num_steps+1]
        yield X, Y
```

验证：

```Python
x = list(range(30))
for x, y in data_iter_consecutive(x, 2, 6):
    print(x)
    print(y)
    print("---")
```

输出：

![image-20211004112703370](src/04.循环神经网络/image-20211004112703370.png)



## 循环神经网络代码实现

本节将会从零开始实现一个基于字符级的循环神经网络的语言模型，用来训练歌词创作。本节中将会使用上一节介绍的数据集加载方法。



### **one hot向量**

为了将词表示成向量输入到神经网络，一个简单的方法是使用one hot向量，假设词典中不同的字符数为**N**(vocab_size)，每个字符都同一个从0到N-1的连续整数一一对应，如果一个字符的索引为i, 我们创建一个值全为0的长度为N的向量，并将其在i位置的值设置为1。

```Python
def one_hot(x, n_class, dtype=torch.float32):
    x = x.long()
    res = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)
    # scatter_(dim, index, src): 将src写到指定的dim中,其索引由第二个参数指定。
    res.scatter_(1, x.view(-1, 1), 1)
    return res
```

验证：

```Python
x = torch.tensor([0, 2, 3])
print(x)
print(one_hot(x, 4))
```

![image-20211004140446856](src/04.循环神经网络/image-20211004140446856.png)

我们再**定义一个将输入转换成one hot向量**的方法：

```Python
def to_onehot(X, n_class):
    # X shape: (batch, seq_len), output: seq_len elements of (batch, n_class)
    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]
```

验证

```Python
    x = torch.arange(10).view(2, 5)
    print(x)
    print(to_onehot(x, 10))
```

输出

![image-20211004151720833](src/04.循环神经网络/image-20211004151720833.png)

以上代码保存在Code/Utils/one_hot.py



### 初始化模型参数

```Python
def get_params():
    # 初始化一个均值为0，标准差为0.01的tensor
    def _one(shape):
        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)
        return torch.nn.Parameter(ts, requires_grad=True)
    # 隐藏层参数
    W_xh = _one((num_inputs, num_hidden))
    W_hh = _one((num_hidden, num_hidden))
    b_h = torch.nn.Parameter(torch.zeros(num_hidden, device=device, requires_grad=True))
    # 输出层参数
    W_hq = _one((num_hidden, num_output))
    b_q = torch.nn.Parameter(torch.zeros(num_output, device=device, requires_grad=True))
    return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])
```

### 定义模型

1. 初始化隐藏状态

我们根据循环神经网络的计算表达式实现该模型，先定义一个函数返回初始化的隐藏状态。隐藏状态的形状为(批量大小，隐藏单元个数)。所有的值都会被初始化为0，这里返回一个元组，是为了便于处理隐藏状态含有多个NDArray的状态。

```Python
def init_rnn_state(batch_size, num_hidden, device):
    return (torch.zeros(batch_size, num_hidden, device=device), )
```

