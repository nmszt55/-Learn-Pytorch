# 循环神经网络

## 语言模型

语言模型是自然语言处理的重要技术，这其中最常见的数据是文本数据，我们可以把自然语言文本看做一段离散的时间序列

假设一段长度为T的文本中的词依次为w1, w2, w3....w~t~, 那么在离散的时间序列中，w~t~可看做在时间部t的输出或者标签，给定一个长度为T的词序列w1, w2, w3....w~t~，语言模型将计算该序列的概率：

![image-20210927213507458](src/04.循环神经网络/image-20210927213507458.png)

语言模型可以作为机器翻译或者语音识别的功能。例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。



### 语言模型的计算

假设w1, w2....w~t~每个词都是依次生成的，我们有

![image-20210927215050934](src/04.循环神经网络/image-20210927215050934.png)

例如：一个含有4个词的文本序列概率

![image-20210927215507377](src/04.循环神经网络/image-20210927215507377.png)

说明：

我们可以使用一个大型的语言文本库，如维基百科的所有条目，词的概率可以通过改词在训练集的词频（词出现的次数）与总词数之比来计算，根据条件概率，一个词在给定前几个词的情况下条件概率可以计算。例如P(w2|w1)可以理解为w1w2相邻的概率除以w1

### n元语法

当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会成指数级增加。n元模型通过**马尔科夫假设**简化了语言模型的计算。

**马尔科夫假设**指的是假设一个词的出现只与前面n个词相关，假设n=1，那么w3的出现只与w2相关，与w1不相关：P(w3|w1,w2) = P(w3|w2)

基于n-1阶马尔科夫假设链，我们可以将语言模型改写为：

![image-20210927232039087](src/04.循环神经网络/image-20210927232039087.png)

上面的语法也称作n元语法，它是基于n-1阶马尔科夫链，当n分别为1,2,3时，也称为**1元语法，2元语法，3元语法**

![image-20210927232442745](src/04.循环神经网络/image-20210927232442745.png)

当n较小时，往往结果并不准确，例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。当n较大时，n元语法需要存储大量的词频和多词相邻概率。有没有方法在语言模型中更好的平衡以上2点呢？那就是**循环神经网络**。



## 循环神经网络-RNN

循环神经网络通过隐藏状态存储之前时间部的信息.类似于之前的多层感知机。

### 不含隐藏状态的神经网络

考虑我们有个单隐藏层的多层感知机，假如激活函数为*ϕ*，那么隐藏层的输出为

![image-20210930145658621](src/04.循环神经网络/image-20210930145658621.png)

隐藏层权重参数W~xh~ ∈ R^nxh^, 偏差函数b ∈ R^1xh^,h为隐藏单元数，经过输出层：

![image-20210930152302222](src/04.循环神经网络/image-20210930152302222.png)

我们通过softmax函数可以进行分类计算



### 含隐藏状态的循环神经网络

假设时间步为t, 我们基于多层感知机，引入一个隐藏状态进行计算：

![image-20210930153835488](src/04.循环神经网络/image-20210930153835488.png)

这个隐藏状态包含了一个新的权重参数W~hh~, 代表在时间步t-1的计算结果，也就是说当前Ht的计算也同时依赖于上一步的计算值，H~t-1~同理依赖H~t-2~的计算结果。那么我们就构造了一个带有隐藏状态的循环神经网路。

最后，我们在加上一层输出层就ok了

![image-20210930164838861](src/04.循环神经网络/image-20210930164838861.png)

**循环神经网络的参数**

循环神经网络的参数包括隐藏层的W~xh~ W~hh~,和偏差b~h~,还包括输出层的W~hq~和B~q~,**即使是在不同的时间步，循环神经网络也依然使用这些参数**，因此它的参数数量不随着时间步的增加而增长。

**循环神经网络的结构**

![image-20211001112234477](src/04.循环神经网络/image-20211001112234477.png)

如图展示了三个时间步内的计算逻辑，在时间步t中，隐藏状态的计算可以看成是将输入与H~t-1~联结后输出到激活函数的全连接层。

**示例**：

```Python
import torch

X, W_xh = torch.randn(3, 1), torch.randn(1, 4)
H, W_hh = torch.randn(3, 4), torch.randn(4, 4)
torch.matmul(X, W_xh) + torch.matmul(H, W_hh)
```

我们的输入形状如果为(3,1), 权重参数形状为(1,4),他们计算出来的结果形状为(3,4)，此时也就限制了H~t-1~的形状和W~hh~的形状。H~t-1~的形状必须为(3,4), W~hh~必须保证H~t-1~与其计算后形状仍是H~t-1~的形状。

![image-20211003000329790](src/04.循环神经网络/image-20211003000329790.png)

优化计算：其实我们将X和H~t-1~在列维度(dim=1)进行相连，形状为(3,1) + (3, 4) = (3,5), 

W_xh和W_hh在行维度相连，形状为(1,4)+(4,4) = (5,4)，

我们将两个组合后的结果进行运算，还是同样得到一个(3,4)的结果，其结果与上面的计算一致

```Python
torch.matmul(torch.cat((X, H), dim=1), torch.cat((W_xh, W_hh), dim=0))
```





### 应用:基于字符级循环神经网络的语言模型

